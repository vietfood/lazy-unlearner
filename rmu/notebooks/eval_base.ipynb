{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c37e7f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-01:17:06:26,578 INFO     [lm_eval.__main__:379] Selected Tasks: ['mmlu', 'wmdp']\n",
      "2025-06-01:17:06:26,580 INFO     [lm_eval.evaluator:169] Setting random seed to 42 | Setting numpy seed to 42 | Setting torch manual seed to 42 | Setting fewshot manual seed to 42\n",
      "2025-06-01:17:06:26,580 INFO     [lm_eval.evaluator:206] Initializing hf model, with arguments: {'pretrained': 'google/gemma-2-2b', 'dtype': 'bfloat16', 'trust_remote_code': True}\n",
      "2025-06-01:17:06:26,665 INFO     [lm_eval.models.huggingface:136] Using device 'cuda'\n",
      "2025-06-01:17:06:28,282 INFO     [lm_eval.models.huggingface:376] Model parallel was set to False, max memory was not set, and device map was set to {'': 'cuda'}\n",
      "Loading checkpoint shards: 100%|██████████████████| 3/3 [00:02<00:00,  1.47it/s]\n",
      "2025-06-01:17:06:30,670 INFO     [lm_eval.models.huggingface:223] Model type is 'gemma2', part of the Gemma family--a BOS token will be used as Gemma underperforms without it.\n",
      "2025-06-01:17:06:45,587 WARNING  [lm_eval.api.task:327] [Task: wmdp_cyber] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.\n",
      "2025-06-01:17:06:45,587 WARNING  [lm_eval.api.task:327] [Task: wmdp_cyber] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.\n",
      "2025-06-01:17:06:46,026 WARNING  [lm_eval.api.task:327] [Task: wmdp_chem] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.\n",
      "2025-06-01:17:06:46,026 WARNING  [lm_eval.api.task:327] [Task: wmdp_chem] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.\n",
      "2025-06-01:17:06:46,359 WARNING  [lm_eval.api.task:327] [Task: wmdp_bio] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.\n",
      "2025-06-01:17:06:46,359 WARNING  [lm_eval.api.task:327] [Task: wmdp_bio] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.\n",
      "2025-06-01:17:06:46,405 INFO     [lm_eval.api.task:420] Building contexts for wmdp_bio on rank 0...\n",
      "100%|██████████████████████████████████████| 1273/1273 [00:01<00:00, 769.76it/s]\n",
      "2025-06-01:17:06:48,101 INFO     [lm_eval.api.task:420] Building contexts for wmdp_chem on rank 0...\n",
      "100%|████████████████████████████████████████| 408/408 [00:00<00:00, 776.53it/s]\n",
      "2025-06-01:17:06:48,640 INFO     [lm_eval.api.task:420] Building contexts for wmdp_cyber on rank 0...\n",
      "100%|██████████████████████████████████████| 1987/1987 [00:02<00:00, 705.75it/s]\n",
      "2025-06-01:17:06:51,521 INFO     [lm_eval.api.task:420] Building contexts for mmlu_college_chemistry on rank 0...\n",
      "100%|████████████████████████████████████████| 100/100 [00:00<00:00, 763.39it/s]\n",
      "2025-06-01:17:06:51,657 INFO     [lm_eval.api.task:420] Building contexts for mmlu_college_biology on rank 0...\n",
      "100%|████████████████████████████████████████| 144/144 [00:00<00:00, 767.91it/s]\n",
      "2025-06-01:17:06:51,850 INFO     [lm_eval.api.task:420] Building contexts for mmlu_high_school_computer_science on rank 0...\n",
      "100%|████████████████████████████████████████| 100/100 [00:00<00:00, 771.82it/s]\n",
      "2025-06-01:17:06:51,985 INFO     [lm_eval.api.task:420] Building contexts for mmlu_electrical_engineering on rank 0...\n",
      "100%|████████████████████████████████████████| 145/145 [00:00<00:00, 763.68it/s]\n",
      "2025-06-01:17:06:52,181 INFO     [lm_eval.api.task:420] Building contexts for mmlu_elementary_mathematics on rank 0...\n",
      "100%|████████████████████████████████████████| 378/378 [00:00<00:00, 775.93it/s]\n",
      "2025-06-01:17:06:52,684 INFO     [lm_eval.api.task:420] Building contexts for mmlu_high_school_statistics on rank 0...\n",
      "100%|████████████████████████████████████████| 216/216 [00:00<00:00, 777.25it/s]\n",
      "2025-06-01:17:06:52,971 INFO     [lm_eval.api.task:420] Building contexts for mmlu_high_school_physics on rank 0...\n",
      "100%|████████████████████████████████████████| 151/151 [00:00<00:00, 774.93it/s]\n",
      "2025-06-01:17:06:53,172 INFO     [lm_eval.api.task:420] Building contexts for mmlu_machine_learning on rank 0...\n",
      "100%|████████████████████████████████████████| 112/112 [00:00<00:00, 776.97it/s]\n",
      "2025-06-01:17:06:53,321 INFO     [lm_eval.api.task:420] Building contexts for mmlu_abstract_algebra on rank 0...\n",
      "100%|████████████████████████████████████████| 100/100 [00:00<00:00, 777.43it/s]\n",
      "2025-06-01:17:06:53,454 INFO     [lm_eval.api.task:420] Building contexts for mmlu_astronomy on rank 0...\n",
      "100%|████████████████████████████████████████| 152/152 [00:00<00:00, 775.35it/s]\n",
      "2025-06-01:17:06:53,657 INFO     [lm_eval.api.task:420] Building contexts for mmlu_computer_security on rank 0...\n",
      "100%|████████████████████████████████████████| 100/100 [00:00<00:00, 779.33it/s]\n",
      "2025-06-01:17:06:53,790 INFO     [lm_eval.api.task:420] Building contexts for mmlu_high_school_chemistry on rank 0...\n",
      "100%|████████████████████████████████████████| 203/203 [00:00<00:00, 769.12it/s]\n",
      "2025-06-01:17:06:54,063 INFO     [lm_eval.api.task:420] Building contexts for mmlu_college_physics on rank 0...\n",
      "100%|████████████████████████████████████████| 102/102 [00:00<00:00, 777.91it/s]\n",
      "2025-06-01:17:06:54,198 INFO     [lm_eval.api.task:420] Building contexts for mmlu_high_school_mathematics on rank 0...\n",
      "100%|████████████████████████████████████████| 270/270 [00:00<00:00, 778.40it/s]\n",
      "2025-06-01:17:06:54,556 INFO     [lm_eval.api.task:420] Building contexts for mmlu_college_mathematics on rank 0...\n",
      "100%|████████████████████████████████████████| 100/100 [00:00<00:00, 775.51it/s]\n",
      "2025-06-01:17:06:54,690 INFO     [lm_eval.api.task:420] Building contexts for mmlu_high_school_biology on rank 0...\n",
      "100%|████████████████████████████████████████| 310/310 [00:00<00:00, 779.68it/s]\n",
      "2025-06-01:17:06:55,100 INFO     [lm_eval.api.task:420] Building contexts for mmlu_college_computer_science on rank 0...\n",
      "100%|████████████████████████████████████████| 100/100 [00:00<00:00, 773.45it/s]\n",
      "2025-06-01:17:06:55,234 INFO     [lm_eval.api.task:420] Building contexts for mmlu_conceptual_physics on rank 0...\n",
      "100%|████████████████████████████████████████| 235/235 [00:00<00:00, 777.13it/s]\n",
      "2025-06-01:17:06:55,546 INFO     [lm_eval.api.task:420] Building contexts for mmlu_anatomy on rank 0...\n",
      "100%|████████████████████████████████████████| 135/135 [00:00<00:00, 777.82it/s]\n",
      "2025-06-01:17:06:55,725 INFO     [lm_eval.api.task:420] Building contexts for mmlu_professional_accounting on rank 0...\n",
      "100%|████████████████████████████████████████| 282/282 [00:00<00:00, 772.79it/s]\n",
      "2025-06-01:17:06:56,102 INFO     [lm_eval.api.task:420] Building contexts for mmlu_clinical_knowledge on rank 0...\n",
      "100%|████████████████████████████████████████| 265/265 [00:00<00:00, 762.30it/s]\n",
      "2025-06-01:17:06:56,461 INFO     [lm_eval.api.task:420] Building contexts for mmlu_marketing on rank 0...\n",
      "100%|████████████████████████████████████████| 234/234 [00:00<00:00, 779.53it/s]\n",
      "2025-06-01:17:06:56,770 INFO     [lm_eval.api.task:420] Building contexts for mmlu_nutrition on rank 0...\n",
      "100%|████████████████████████████████████████| 306/306 [00:00<00:00, 780.30it/s]\n",
      "2025-06-01:17:06:57,175 INFO     [lm_eval.api.task:420] Building contexts for mmlu_medical_genetics on rank 0...\n",
      "100%|████████████████████████████████████████| 100/100 [00:00<00:00, 780.43it/s]\n",
      "2025-06-01:17:06:57,307 INFO     [lm_eval.api.task:420] Building contexts for mmlu_business_ethics on rank 0...\n",
      "100%|████████████████████████████████████████| 100/100 [00:00<00:00, 778.97it/s]\n",
      "2025-06-01:17:06:57,440 INFO     [lm_eval.api.task:420] Building contexts for mmlu_professional_medicine on rank 0...\n",
      "100%|████████████████████████████████████████| 272/272 [00:00<00:00, 778.12it/s]\n",
      "2025-06-01:17:06:57,802 INFO     [lm_eval.api.task:420] Building contexts for mmlu_miscellaneous on rank 0...\n",
      "100%|████████████████████████████████████████| 783/783 [00:01<00:00, 779.44it/s]\n",
      "2025-06-01:17:06:58,837 INFO     [lm_eval.api.task:420] Building contexts for mmlu_management on rank 0...\n",
      "100%|████████████████████████████████████████| 103/103 [00:00<00:00, 778.61it/s]\n",
      "2025-06-01:17:06:58,974 INFO     [lm_eval.api.task:420] Building contexts for mmlu_virology on rank 0...\n",
      "100%|████████████████████████████████████████| 166/166 [00:00<00:00, 778.12it/s]\n",
      "2025-06-01:17:06:59,195 INFO     [lm_eval.api.task:420] Building contexts for mmlu_college_medicine on rank 0...\n",
      "100%|████████████████████████████████████████| 173/173 [00:00<00:00, 777.78it/s]\n",
      "2025-06-01:17:06:59,425 INFO     [lm_eval.api.task:420] Building contexts for mmlu_global_facts on rank 0...\n",
      "100%|████████████████████████████████████████| 100/100 [00:00<00:00, 774.91it/s]\n",
      "2025-06-01:17:06:59,558 INFO     [lm_eval.api.task:420] Building contexts for mmlu_human_aging on rank 0...\n",
      "100%|████████████████████████████████████████| 223/223 [00:00<00:00, 775.15it/s]\n",
      "2025-06-01:17:06:59,855 INFO     [lm_eval.api.task:420] Building contexts for mmlu_high_school_microeconomics on rank 0...\n",
      "100%|████████████████████████████████████████| 238/238 [00:00<00:00, 776.15it/s]\n",
      "2025-06-01:17:07:00,172 INFO     [lm_eval.api.task:420] Building contexts for mmlu_security_studies on rank 0...\n",
      "100%|████████████████████████████████████████| 245/245 [00:00<00:00, 776.93it/s]\n",
      "2025-06-01:17:07:00,497 INFO     [lm_eval.api.task:420] Building contexts for mmlu_econometrics on rank 0...\n",
      "100%|████████████████████████████████████████| 114/114 [00:00<00:00, 781.47it/s]\n",
      "2025-06-01:17:07:00,648 INFO     [lm_eval.api.task:420] Building contexts for mmlu_us_foreign_policy on rank 0...\n",
      "100%|████████████████████████████████████████| 100/100 [00:00<00:00, 777.64it/s]\n",
      "2025-06-01:17:07:00,781 INFO     [lm_eval.api.task:420] Building contexts for mmlu_public_relations on rank 0...\n",
      "100%|████████████████████████████████████████| 110/110 [00:00<00:00, 777.03it/s]\n",
      "2025-06-01:17:07:00,928 INFO     [lm_eval.api.task:420] Building contexts for mmlu_high_school_macroeconomics on rank 0...\n",
      "100%|████████████████████████████████████████| 390/390 [00:00<00:00, 779.02it/s]\n",
      "2025-06-01:17:07:01,444 INFO     [lm_eval.api.task:420] Building contexts for mmlu_high_school_geography on rank 0...\n",
      "100%|████████████████████████████████████████| 198/198 [00:00<00:00, 779.53it/s]\n",
      "2025-06-01:17:07:01,707 INFO     [lm_eval.api.task:420] Building contexts for mmlu_high_school_psychology on rank 0...\n",
      "100%|████████████████████████████████████████| 545/545 [00:00<00:00, 603.13it/s]\n",
      "2025-06-01:17:07:02,632 INFO     [lm_eval.api.task:420] Building contexts for mmlu_human_sexuality on rank 0...\n",
      "100%|████████████████████████████████████████| 131/131 [00:00<00:00, 782.55it/s]\n",
      "2025-06-01:17:07:02,805 INFO     [lm_eval.api.task:420] Building contexts for mmlu_professional_psychology on rank 0...\n",
      "100%|████████████████████████████████████████| 612/612 [00:00<00:00, 783.12it/s]\n",
      "2025-06-01:17:07:03,611 INFO     [lm_eval.api.task:420] Building contexts for mmlu_high_school_government_and_politics on rank 0...\n",
      "100%|████████████████████████████████████████| 193/193 [00:00<00:00, 779.33it/s]\n",
      "2025-06-01:17:07:03,867 INFO     [lm_eval.api.task:420] Building contexts for mmlu_sociology on rank 0...\n",
      "100%|████████████████████████████████████████| 201/201 [00:00<00:00, 777.89it/s]\n",
      "2025-06-01:17:07:04,134 INFO     [lm_eval.api.task:420] Building contexts for mmlu_prehistory on rank 0...\n",
      "100%|████████████████████████████████████████| 324/324 [00:00<00:00, 780.45it/s]\n",
      "2025-06-01:17:07:04,562 INFO     [lm_eval.api.task:420] Building contexts for mmlu_jurisprudence on rank 0...\n",
      "100%|████████████████████████████████████████| 108/108 [00:00<00:00, 776.44it/s]\n",
      "2025-06-01:17:07:04,706 INFO     [lm_eval.api.task:420] Building contexts for mmlu_professional_law on rank 0...\n",
      "100%|██████████████████████████████████████| 1534/1534 [00:01<00:00, 777.42it/s]\n",
      "2025-06-01:17:07:06,742 INFO     [lm_eval.api.task:420] Building contexts for mmlu_high_school_world_history on rank 0...\n",
      "100%|████████████████████████████████████████| 237/237 [00:00<00:00, 783.07it/s]\n",
      "2025-06-01:17:07:07,055 INFO     [lm_eval.api.task:420] Building contexts for mmlu_moral_scenarios on rank 0...\n",
      "100%|████████████████████████████████████████| 895/895 [00:01<00:00, 785.60it/s]\n",
      "2025-06-01:17:07:08,230 INFO     [lm_eval.api.task:420] Building contexts for mmlu_formal_logic on rank 0...\n",
      "100%|████████████████████████████████████████| 126/126 [00:00<00:00, 782.20it/s]\n",
      "2025-06-01:17:07:08,397 INFO     [lm_eval.api.task:420] Building contexts for mmlu_international_law on rank 0...\n",
      "100%|████████████████████████████████████████| 121/121 [00:00<00:00, 781.85it/s]\n",
      "2025-06-01:17:07:08,557 INFO     [lm_eval.api.task:420] Building contexts for mmlu_philosophy on rank 0...\n",
      "100%|████████████████████████████████████████| 311/311 [00:00<00:00, 787.24it/s]\n",
      "2025-06-01:17:07:08,965 INFO     [lm_eval.api.task:420] Building contexts for mmlu_high_school_european_history on rank 0...\n",
      "100%|████████████████████████████████████████| 165/165 [00:00<00:00, 777.66it/s]\n",
      "2025-06-01:17:07:09,185 INFO     [lm_eval.api.task:420] Building contexts for mmlu_world_religions on rank 0...\n",
      "100%|████████████████████████████████████████| 171/171 [00:00<00:00, 783.94it/s]\n",
      "2025-06-01:17:07:09,410 INFO     [lm_eval.api.task:420] Building contexts for mmlu_moral_disputes on rank 0...\n",
      "100%|████████████████████████████████████████| 346/346 [00:00<00:00, 785.80it/s]\n",
      "2025-06-01:17:07:09,864 INFO     [lm_eval.api.task:420] Building contexts for mmlu_high_school_us_history on rank 0...\n",
      "100%|████████████████████████████████████████| 204/204 [00:00<00:00, 779.77it/s]\n",
      "2025-06-01:17:07:10,135 INFO     [lm_eval.api.task:420] Building contexts for mmlu_logical_fallacies on rank 0...\n",
      "100%|████████████████████████████████████████| 163/163 [00:00<00:00, 786.09it/s]\n",
      "2025-06-01:17:07:10,349 INFO     [lm_eval.evaluator:517] Running loglikelihood requests\n",
      "Running loglikelihood requests: 100%|████| 70840/70840 [04:17<00:00, 275.23it/s]\n",
      "2025-06-01:17:12:14,496 INFO     [lm_eval.loggers.evaluation_tracker:209] Saving results aggregated\n",
      "2025-06-01:17:12:14,510 INFO     [lm_eval.loggers.evaluation_tracker:290] Saving per-sample results for: mmlu_abstract_algebra\n",
      "2025-06-01:17:12:14,518 INFO     [lm_eval.loggers.evaluation_tracker:290] Saving per-sample results for: mmlu_anatomy\n",
      "2025-06-01:17:12:14,528 INFO     [lm_eval.loggers.evaluation_tracker:290] Saving per-sample results for: mmlu_astronomy\n",
      "2025-06-01:17:12:14,539 INFO     [lm_eval.loggers.evaluation_tracker:290] Saving per-sample results for: mmlu_business_ethics\n",
      "2025-06-01:17:12:14,546 INFO     [lm_eval.loggers.evaluation_tracker:290] Saving per-sample results for: mmlu_clinical_knowledge\n",
      "2025-06-01:17:12:14,565 INFO     [lm_eval.loggers.evaluation_tracker:290] Saving per-sample results for: mmlu_college_biology\n",
      "2025-06-01:17:12:14,576 INFO     [lm_eval.loggers.evaluation_tracker:290] Saving per-sample results for: mmlu_college_chemistry\n",
      "2025-06-01:17:12:14,584 INFO     [lm_eval.loggers.evaluation_tracker:290] Saving per-sample results for: mmlu_college_computer_science\n",
      "2025-06-01:17:12:14,592 INFO     [lm_eval.loggers.evaluation_tracker:290] Saving per-sample results for: mmlu_college_mathematics\n",
      "2025-06-01:17:12:14,600 INFO     [lm_eval.loggers.evaluation_tracker:290] Saving per-sample results for: mmlu_college_medicine\n",
      "2025-06-01:17:12:15,094 INFO     [lm_eval.loggers.evaluation_tracker:290] Saving per-sample results for: mmlu_college_physics\n",
      "2025-06-01:17:12:15,102 INFO     [lm_eval.loggers.evaluation_tracker:290] Saving per-sample results for: mmlu_computer_security\n",
      "2025-06-01:17:12:15,110 INFO     [lm_eval.loggers.evaluation_tracker:290] Saving per-sample results for: mmlu_conceptual_physics\n",
      "2025-06-01:17:12:15,125 INFO     [lm_eval.loggers.evaluation_tracker:290] Saving per-sample results for: mmlu_econometrics\n",
      "2025-06-01:17:12:15,134 INFO     [lm_eval.loggers.evaluation_tracker:290] Saving per-sample results for: mmlu_electrical_engineering\n",
      "2025-06-01:17:12:15,144 INFO     [lm_eval.loggers.evaluation_tracker:290] Saving per-sample results for: mmlu_elementary_mathematics\n",
      "2025-06-01:17:12:15,177 INFO     [lm_eval.loggers.evaluation_tracker:290] Saving per-sample results for: mmlu_formal_logic\n",
      "2025-06-01:17:12:15,188 INFO     [lm_eval.loggers.evaluation_tracker:290] Saving per-sample results for: mmlu_global_facts\n",
      "2025-06-01:17:12:15,195 INFO     [lm_eval.loggers.evaluation_tracker:290] Saving per-sample results for: mmlu_high_school_biology\n",
      "2025-06-01:17:12:15,222 INFO     [lm_eval.loggers.evaluation_tracker:290] Saving per-sample results for: mmlu_high_school_chemistry\n",
      "2025-06-01:17:12:15,238 INFO     [lm_eval.loggers.evaluation_tracker:290] Saving per-sample results for: mmlu_high_school_computer_science\n",
      "2025-06-01:17:12:15,247 INFO     [lm_eval.loggers.evaluation_tracker:290] Saving per-sample results for: mmlu_high_school_european_history\n",
      "2025-06-01:17:12:15,270 INFO     [lm_eval.loggers.evaluation_tracker:290] Saving per-sample results for: mmlu_high_school_geography\n",
      "2025-06-01:17:12:15,284 INFO     [lm_eval.loggers.evaluation_tracker:290] Saving per-sample results for: mmlu_high_school_government_and_politics\n",
      "2025-06-01:17:12:15,299 INFO     [lm_eval.loggers.evaluation_tracker:290] Saving per-sample results for: mmlu_high_school_macroeconomics\n",
      "2025-06-01:17:12:15,329 INFO     [lm_eval.loggers.evaluation_tracker:290] Saving per-sample results for: mmlu_high_school_mathematics\n",
      "2025-06-01:17:12:15,349 INFO     [lm_eval.loggers.evaluation_tracker:290] Saving per-sample results for: mmlu_high_school_microeconomics\n",
      "2025-06-01:17:12:15,366 INFO     [lm_eval.loggers.evaluation_tracker:290] Saving per-sample results for: mmlu_high_school_physics\n",
      "2025-06-01:17:12:15,382 INFO     [lm_eval.loggers.evaluation_tracker:290] Saving per-sample results for: mmlu_high_school_psychology\n",
      "2025-06-01:17:12:15,420 INFO     [lm_eval.loggers.evaluation_tracker:290] Saving per-sample results for: mmlu_high_school_statistics\n",
      "2025-06-01:17:12:15,438 INFO     [lm_eval.loggers.evaluation_tracker:290] Saving per-sample results for: mmlu_high_school_us_history\n",
      "2025-06-01:17:12:15,466 INFO     [lm_eval.loggers.evaluation_tracker:290] Saving per-sample results for: mmlu_high_school_world_history\n",
      "2025-06-01:17:12:15,498 INFO     [lm_eval.loggers.evaluation_tracker:290] Saving per-sample results for: mmlu_human_aging\n",
      "2025-06-01:17:12:15,513 INFO     [lm_eval.loggers.evaluation_tracker:290] Saving per-sample results for: mmlu_human_sexuality\n",
      "2025-06-01:17:12:15,522 INFO     [lm_eval.loggers.evaluation_tracker:290] Saving per-sample results for: mmlu_international_law\n",
      "2025-06-01:17:12:15,532 INFO     [lm_eval.loggers.evaluation_tracker:290] Saving per-sample results for: mmlu_jurisprudence\n",
      "2025-06-01:17:12:15,541 INFO     [lm_eval.loggers.evaluation_tracker:290] Saving per-sample results for: mmlu_logical_fallacies\n",
      "2025-06-01:17:12:15,553 INFO     [lm_eval.loggers.evaluation_tracker:290] Saving per-sample results for: mmlu_machine_learning\n",
      "2025-06-01:17:12:15,564 INFO     [lm_eval.loggers.evaluation_tracker:290] Saving per-sample results for: mmlu_management\n",
      "2025-06-01:17:12:15,571 INFO     [lm_eval.loggers.evaluation_tracker:290] Saving per-sample results for: mmlu_marketing\n",
      "2025-06-01:17:12:15,588 INFO     [lm_eval.loggers.evaluation_tracker:290] Saving per-sample results for: mmlu_medical_genetics\n",
      "2025-06-01:17:12:15,595 INFO     [lm_eval.loggers.evaluation_tracker:290] Saving per-sample results for: mmlu_miscellaneous\n",
      "2025-06-01:17:12:15,647 INFO     [lm_eval.loggers.evaluation_tracker:290] Saving per-sample results for: mmlu_moral_disputes\n",
      "2025-06-01:17:12:15,674 INFO     [lm_eval.loggers.evaluation_tracker:290] Saving per-sample results for: mmlu_moral_scenarios\n",
      "2025-06-01:17:12:15,742 INFO     [lm_eval.loggers.evaluation_tracker:290] Saving per-sample results for: mmlu_nutrition\n",
      "2025-06-01:17:12:15,764 INFO     [lm_eval.loggers.evaluation_tracker:290] Saving per-sample results for: mmlu_philosophy\n",
      "2025-06-01:17:12:15,788 INFO     [lm_eval.loggers.evaluation_tracker:290] Saving per-sample results for: mmlu_prehistory\n",
      "2025-06-01:17:12:15,812 INFO     [lm_eval.loggers.evaluation_tracker:290] Saving per-sample results for: mmlu_professional_accounting\n",
      "2025-06-01:17:12:15,835 INFO     [lm_eval.loggers.evaluation_tracker:290] Saving per-sample results for: mmlu_professional_law\n",
      "2025-06-01:17:12:15,997 INFO     [lm_eval.loggers.evaluation_tracker:290] Saving per-sample results for: mmlu_professional_medicine\n",
      "2025-06-01:17:12:16,022 INFO     [lm_eval.loggers.evaluation_tracker:290] Saving per-sample results for: mmlu_professional_psychology\n",
      "2025-06-01:17:12:16,069 INFO     [lm_eval.loggers.evaluation_tracker:290] Saving per-sample results for: mmlu_public_relations\n",
      "2025-06-01:17:12:16,077 INFO     [lm_eval.loggers.evaluation_tracker:290] Saving per-sample results for: mmlu_security_studies\n",
      "2025-06-01:17:12:16,102 INFO     [lm_eval.loggers.evaluation_tracker:290] Saving per-sample results for: mmlu_sociology\n",
      "2025-06-01:17:12:16,117 INFO     [lm_eval.loggers.evaluation_tracker:290] Saving per-sample results for: mmlu_us_foreign_policy\n",
      "2025-06-01:17:12:16,125 INFO     [lm_eval.loggers.evaluation_tracker:290] Saving per-sample results for: mmlu_virology\n",
      "2025-06-01:17:12:16,137 INFO     [lm_eval.loggers.evaluation_tracker:290] Saving per-sample results for: mmlu_world_religions\n",
      "2025-06-01:17:12:16,148 INFO     [lm_eval.loggers.evaluation_tracker:290] Saving per-sample results for: wmdp_bio\n",
      "2025-06-01:17:12:16,247 INFO     [lm_eval.loggers.evaluation_tracker:290] Saving per-sample results for: wmdp_chem\n",
      "2025-06-01:17:12:16,275 INFO     [lm_eval.loggers.evaluation_tracker:290] Saving per-sample results for: wmdp_cyber\n",
      "hf (pretrained=google/gemma-2-2b,dtype=bfloat16,trust_remote_code=True), gen_kwargs: (None), limit: None, num_fewshot: None, batch_size: 8\n",
      "|                 Tasks                 |Version|Filter|n-shot|Metric|   |Value |   |Stderr|\n",
      "|---------------------------------------|------:|------|-----:|------|---|-----:|---|-----:|\n",
      "|mmlu                                   |      2|none  |      |acc   |↑  |0.4937|±  |0.0041|\n",
      "| - humanities                          |      2|none  |      |acc   |↑  |0.4400|±  |0.0070|\n",
      "|  - formal_logic                       |      1|none  |     0|acc   |↑  |0.3175|±  |0.0416|\n",
      "|  - high_school_european_history       |      1|none  |     0|acc   |↑  |0.5818|±  |0.0385|\n",
      "|  - high_school_us_history             |      1|none  |     0|acc   |↑  |0.6078|±  |0.0343|\n",
      "|  - high_school_world_history          |      1|none  |     0|acc   |↑  |0.5992|±  |0.0319|\n",
      "|  - international_law                  |      1|none  |     0|acc   |↑  |0.6364|±  |0.0439|\n",
      "|  - jurisprudence                      |      1|none  |     0|acc   |↑  |0.5556|±  |0.0480|\n",
      "|  - logical_fallacies                  |      1|none  |     0|acc   |↑  |0.5337|±  |0.0392|\n",
      "|  - moral_disputes                     |      1|none  |     0|acc   |↑  |0.5520|±  |0.0268|\n",
      "|  - moral_scenarios                    |      1|none  |     0|acc   |↑  |0.2425|±  |0.0143|\n",
      "|  - philosophy                         |      1|none  |     0|acc   |↑  |0.5402|±  |0.0283|\n",
      "|  - prehistory                         |      1|none  |     0|acc   |↑  |0.5679|±  |0.0276|\n",
      "|  - professional_law                   |      1|none  |     0|acc   |↑  |0.3774|±  |0.0124|\n",
      "|  - world_religions                    |      1|none  |     0|acc   |↑  |0.6140|±  |0.0373|\n",
      "| - other                               |      2|none  |      |acc   |↑  |0.5620|±  |0.0087|\n",
      "|  - business_ethics                    |      1|none  |     0|acc   |↑  |0.4700|±  |0.0502|\n",
      "|  - clinical_knowledge                 |      1|none  |     0|acc   |↑  |0.5547|±  |0.0306|\n",
      "|  - college_medicine                   |      1|none  |     0|acc   |↑  |0.5434|±  |0.0380|\n",
      "|  - global_facts                       |      1|none  |     0|acc   |↑  |0.3100|±  |0.0465|\n",
      "|  - human_aging                        |      1|none  |     0|acc   |↑  |0.5919|±  |0.0330|\n",
      "|  - management                         |      1|none  |     0|acc   |↑  |0.6311|±  |0.0478|\n",
      "|  - marketing                          |      1|none  |     0|acc   |↑  |0.7735|±  |0.0274|\n",
      "|  - medical_genetics                   |      1|none  |     0|acc   |↑  |0.6300|±  |0.0485|\n",
      "|  - miscellaneous                      |      1|none  |     0|acc   |↑  |0.6335|±  |0.0172|\n",
      "|  - nutrition                          |      1|none  |     0|acc   |↑  |0.5752|±  |0.0283|\n",
      "|  - professional_accounting            |      1|none  |     0|acc   |↑  |0.3759|±  |0.0289|\n",
      "|  - professional_medicine              |      1|none  |     0|acc   |↑  |0.4706|±  |0.0303|\n",
      "|  - virology                           |      1|none  |     0|acc   |↑  |0.4819|±  |0.0389|\n",
      "| - social sciences                     |      2|none  |      |acc   |↑  |0.5739|±  |0.0087|\n",
      "|  - econometrics                       |      1|none  |     0|acc   |↑  |0.2982|±  |0.0430|\n",
      "|  - high_school_geography              |      1|none  |     0|acc   |↑  |0.6010|±  |0.0349|\n",
      "|  - high_school_government_and_politics|      1|none  |     0|acc   |↑  |0.6891|±  |0.0334|\n",
      "|  - high_school_macroeconomics         |      1|none  |     0|acc   |↑  |0.4795|±  |0.0253|\n",
      "|  - high_school_microeconomics         |      1|none  |     0|acc   |↑  |0.4874|±  |0.0325|\n",
      "|  - high_school_psychology             |      1|none  |     0|acc   |↑  |0.7064|±  |0.0195|\n",
      "|  - human_sexuality                    |      1|none  |     0|acc   |↑  |0.5649|±  |0.0435|\n",
      "|  - professional_psychology            |      1|none  |     0|acc   |↑  |0.4820|±  |0.0202|\n",
      "|  - public_relations                   |      1|none  |     0|acc   |↑  |0.5364|±  |0.0478|\n",
      "|  - security_studies                   |      1|none  |     0|acc   |↑  |0.6041|±  |0.0313|\n",
      "|  - sociology                          |      1|none  |     0|acc   |↑  |0.7114|±  |0.0320|\n",
      "|  - us_foreign_policy                  |      1|none  |     0|acc   |↑  |0.7300|±  |0.0446|\n",
      "| - stem                                |      2|none  |      |acc   |↑  |0.4285|±  |0.0086|\n",
      "|  - abstract_algebra                   |      1|none  |     0|acc   |↑  |0.2800|±  |0.0451|\n",
      "|  - anatomy                            |      1|none  |     0|acc   |↑  |0.5407|±  |0.0430|\n",
      "|  - astronomy                          |      1|none  |     0|acc   |↑  |0.5263|±  |0.0406|\n",
      "|  - college_biology                    |      1|none  |     0|acc   |↑  |0.5903|±  |0.0411|\n",
      "|  - college_chemistry                  |      1|none  |     0|acc   |↑  |0.4200|±  |0.0496|\n",
      "|  - college_computer_science           |      1|none  |     0|acc   |↑  |0.4400|±  |0.0499|\n",
      "|  - college_mathematics                |      1|none  |     0|acc   |↑  |0.3400|±  |0.0476|\n",
      "|  - college_physics                    |      1|none  |     0|acc   |↑  |0.4020|±  |0.0488|\n",
      "|  - computer_security                  |      1|none  |     0|acc   |↑  |0.5700|±  |0.0498|\n",
      "|  - conceptual_physics                 |      1|none  |     0|acc   |↑  |0.4255|±  |0.0323|\n",
      "|  - electrical_engineering             |      1|none  |     0|acc   |↑  |0.5517|±  |0.0414|\n",
      "|  - elementary_mathematics             |      1|none  |     0|acc   |↑  |0.3175|±  |0.0240|\n",
      "|  - high_school_biology                |      1|none  |     0|acc   |↑  |0.6355|±  |0.0274|\n",
      "|  - high_school_chemistry              |      1|none  |     0|acc   |↑  |0.3941|±  |0.0344|\n",
      "|  - high_school_computer_science       |      1|none  |     0|acc   |↑  |0.4300|±  |0.0498|\n",
      "|  - high_school_mathematics            |      1|none  |     0|acc   |↑  |0.2593|±  |0.0267|\n",
      "|  - high_school_physics                |      1|none  |     0|acc   |↑  |0.3775|±  |0.0396|\n",
      "|  - high_school_statistics             |      1|none  |     0|acc   |↑  |0.3981|±  |0.0334|\n",
      "|  - machine_learning                   |      1|none  |     0|acc   |↑  |0.3036|±  |0.0436|\n",
      "|wmdp                                   |      1|none  |      |acc   |↑  |0.4354|±  |0.0080|\n",
      "| - wmdp_bio                            |      1|none  |     0|acc   |↑  |0.5876|±  |0.0138|\n",
      "| - wmdp_chem                           |      1|none  |     0|acc   |↑  |0.3799|±  |0.0241|\n",
      "| - wmdp_cyber                          |      1|none  |     0|acc   |↑  |0.3493|±  |0.0107|\n",
      "\n",
      "|      Groups      |Version|Filter|n-shot|Metric|   |Value |   |Stderr|\n",
      "|------------------|------:|------|------|------|---|-----:|---|-----:|\n",
      "|mmlu              |      2|none  |      |acc   |↑  |0.4937|±  |0.0041|\n",
      "| - humanities     |      2|none  |      |acc   |↑  |0.4400|±  |0.0070|\n",
      "| - other          |      2|none  |      |acc   |↑  |0.5620|±  |0.0087|\n",
      "| - social sciences|      2|none  |      |acc   |↑  |0.5739|±  |0.0087|\n",
      "| - stem           |      2|none  |      |acc   |↑  |0.4285|±  |0.0086|\n",
      "|wmdp              |      1|none  |      |acc   |↑  |0.4354|±  |0.0080|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!uv run lm-eval --model hf \\\n",
    "    --model_args pretrained=\"google/gemma-2-2b\",dtype=bfloat16,trust_remote_code=True \\\n",
    "    --tasks mmlu,wmdp \\\n",
    "    --log_samples \\\n",
    "    --output_path \"eval/gemma-2b\" \\\n",
    "    --batch_size=8 \\\n",
    "    --seed 42"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
