{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "105c09ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-01:16:44:25,563 INFO     [lm_eval.__main__:379] Selected Tasks: ['mmlu', 'wmdp']\n",
      "2025-06-01:16:44:25,565 INFO     [lm_eval.evaluator:169] Setting random seed to 42 | Setting numpy seed to 42 | Setting torch manual seed to 42 | Setting fewshot manual seed to 42\n",
      "2025-06-01:16:44:25,565 INFO     [lm_eval.evaluator:206] Initializing hf model, with arguments: {'pretrained': '../rmu_results/layer9_alpha300.0_steer350.0', 'dtype': 'bfloat16'}\n",
      "2025-06-01:16:44:25,647 INFO     [lm_eval.models.huggingface:136] Using device 'cuda'\n",
      "2025-06-01:16:44:29,222 INFO     [lm_eval.models.huggingface:376] Model parallel was set to False, max memory was not set, and device map was set to {'': 'cuda'}\n",
      "Loading checkpoint shards: 100%|██████████████████| 3/3 [00:02<00:00,  1.41it/s]\n",
      "2025-06-01:16:44:31,560 INFO     [lm_eval.models.huggingface:223] Model type is 'gemma2', part of the Gemma family--a BOS token will be used as Gemma underperforms without it.\n",
      "2025-06-01:16:44:45,797 WARNING  [lm_eval.api.task:327] [Task: wmdp_cyber] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.\n",
      "2025-06-01:16:44:45,797 WARNING  [lm_eval.api.task:327] [Task: wmdp_cyber] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.\n",
      "2025-06-01:16:44:46,299 WARNING  [lm_eval.api.task:327] [Task: wmdp_chem] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.\n",
      "2025-06-01:16:44:46,299 WARNING  [lm_eval.api.task:327] [Task: wmdp_chem] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.\n",
      "2025-06-01:16:44:46,863 WARNING  [lm_eval.api.task:327] [Task: wmdp_bio] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.\n",
      "2025-06-01:16:44:46,863 WARNING  [lm_eval.api.task:327] [Task: wmdp_bio] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.\n",
      "2025-06-01:16:44:46,908 INFO     [lm_eval.api.task:420] Building contexts for wmdp_bio on rank 0...\n",
      "100%|██████████████████████████████████████| 1273/1273 [00:01<00:00, 784.37it/s]\n",
      "2025-06-01:16:44:48,572 INFO     [lm_eval.api.task:420] Building contexts for wmdp_chem on rank 0...\n",
      "100%|████████████████████████████████████████| 408/408 [00:00<00:00, 789.84it/s]\n",
      "2025-06-01:16:44:49,102 INFO     [lm_eval.api.task:420] Building contexts for wmdp_cyber on rank 0...\n",
      "100%|██████████████████████████████████████| 1987/1987 [00:02<00:00, 788.60it/s]\n",
      "2025-06-01:16:44:51,685 INFO     [lm_eval.api.task:420] Building contexts for mmlu_college_chemistry on rank 0...\n",
      "100%|████████████████████████████████████████| 100/100 [00:00<00:00, 786.14it/s]\n",
      "2025-06-01:16:44:51,817 INFO     [lm_eval.api.task:420] Building contexts for mmlu_college_biology on rank 0...\n",
      "100%|████████████████████████████████████████| 144/144 [00:00<00:00, 783.11it/s]\n",
      "2025-06-01:16:44:52,007 INFO     [lm_eval.api.task:420] Building contexts for mmlu_high_school_computer_science on rank 0...\n",
      "100%|████████████████████████████████████████| 100/100 [00:00<00:00, 784.67it/s]\n",
      "2025-06-01:16:44:52,139 INFO     [lm_eval.api.task:420] Building contexts for mmlu_electrical_engineering on rank 0...\n",
      "100%|████████████████████████████████████████| 145/145 [00:00<00:00, 785.10it/s]\n",
      "2025-06-01:16:44:52,329 INFO     [lm_eval.api.task:420] Building contexts for mmlu_elementary_mathematics on rank 0...\n",
      "100%|████████████████████████████████████████| 378/378 [00:00<00:00, 786.93it/s]\n",
      "2025-06-01:16:44:52,824 INFO     [lm_eval.api.task:420] Building contexts for mmlu_high_school_statistics on rank 0...\n",
      "100%|████████████████████████████████████████| 216/216 [00:00<00:00, 782.54it/s]\n",
      "2025-06-01:16:44:53,110 INFO     [lm_eval.api.task:420] Building contexts for mmlu_high_school_physics on rank 0...\n",
      "100%|████████████████████████████████████████| 151/151 [00:00<00:00, 787.51it/s]\n",
      "2025-06-01:16:44:53,309 INFO     [lm_eval.api.task:420] Building contexts for mmlu_machine_learning on rank 0...\n",
      "100%|████████████████████████████████████████| 112/112 [00:00<00:00, 783.93it/s]\n",
      "2025-06-01:16:44:53,456 INFO     [lm_eval.api.task:420] Building contexts for mmlu_abstract_algebra on rank 0...\n",
      "100%|████████████████████████████████████████| 100/100 [00:00<00:00, 789.40it/s]\n",
      "2025-06-01:16:44:53,587 INFO     [lm_eval.api.task:420] Building contexts for mmlu_astronomy on rank 0...\n",
      "100%|████████████████████████████████████████| 152/152 [00:00<00:00, 793.92it/s]\n",
      "2025-06-01:16:44:53,785 INFO     [lm_eval.api.task:420] Building contexts for mmlu_computer_security on rank 0...\n",
      "100%|████████████████████████████████████████| 100/100 [00:00<00:00, 785.47it/s]\n",
      "2025-06-01:16:44:53,917 INFO     [lm_eval.api.task:420] Building contexts for mmlu_high_school_chemistry on rank 0...\n",
      "100%|████████████████████████████████████████| 203/203 [00:00<00:00, 789.22it/s]\n",
      "2025-06-01:16:44:54,182 INFO     [lm_eval.api.task:420] Building contexts for mmlu_college_physics on rank 0...\n",
      "100%|████████████████████████████████████████| 102/102 [00:00<00:00, 793.81it/s]\n",
      "2025-06-01:16:44:54,315 INFO     [lm_eval.api.task:420] Building contexts for mmlu_high_school_mathematics on rank 0...\n",
      "100%|████████████████████████████████████████| 270/270 [00:00<00:00, 788.56it/s]\n",
      "2025-06-01:16:44:54,668 INFO     [lm_eval.api.task:420] Building contexts for mmlu_college_mathematics on rank 0...\n",
      "100%|████████████████████████████████████████| 100/100 [00:00<00:00, 787.92it/s]\n",
      "2025-06-01:16:44:54,799 INFO     [lm_eval.api.task:420] Building contexts for mmlu_high_school_biology on rank 0...\n",
      "100%|████████████████████████████████████████| 310/310 [00:00<00:00, 494.75it/s]\n",
      "2025-06-01:16:44:55,439 INFO     [lm_eval.api.task:420] Building contexts for mmlu_college_computer_science on rank 0...\n",
      "100%|████████████████████████████████████████| 100/100 [00:00<00:00, 778.76it/s]\n",
      "2025-06-01:16:44:55,572 INFO     [lm_eval.api.task:420] Building contexts for mmlu_conceptual_physics on rank 0...\n",
      "100%|████████████████████████████████████████| 235/235 [00:00<00:00, 751.55it/s]\n",
      "2025-06-01:16:44:55,894 INFO     [lm_eval.api.task:420] Building contexts for mmlu_anatomy on rank 0...\n",
      "100%|████████████████████████████████████████| 135/135 [00:00<00:00, 785.79it/s]\n",
      "2025-06-01:16:44:56,071 INFO     [lm_eval.api.task:420] Building contexts for mmlu_professional_accounting on rank 0...\n",
      "100%|████████████████████████████████████████| 282/282 [00:00<00:00, 776.55it/s]\n",
      "2025-06-01:16:44:56,446 INFO     [lm_eval.api.task:420] Building contexts for mmlu_clinical_knowledge on rank 0...\n",
      "100%|████████████████████████████████████████| 265/265 [00:00<00:00, 775.88it/s]\n",
      "2025-06-01:16:44:56,798 INFO     [lm_eval.api.task:420] Building contexts for mmlu_marketing on rank 0...\n",
      "100%|████████████████████████████████████████| 234/234 [00:00<00:00, 783.82it/s]\n",
      "2025-06-01:16:44:57,106 INFO     [lm_eval.api.task:420] Building contexts for mmlu_nutrition on rank 0...\n",
      "100%|████████████████████████████████████████| 306/306 [00:00<00:00, 783.41it/s]\n",
      "2025-06-01:16:44:57,509 INFO     [lm_eval.api.task:420] Building contexts for mmlu_medical_genetics on rank 0...\n",
      "100%|████████████████████████████████████████| 100/100 [00:00<00:00, 783.67it/s]\n",
      "2025-06-01:16:44:57,641 INFO     [lm_eval.api.task:420] Building contexts for mmlu_business_ethics on rank 0...\n",
      "100%|████████████████████████████████████████| 100/100 [00:00<00:00, 778.68it/s]\n",
      "2025-06-01:16:44:57,774 INFO     [lm_eval.api.task:420] Building contexts for mmlu_professional_medicine on rank 0...\n",
      "100%|████████████████████████████████████████| 272/272 [00:00<00:00, 780.39it/s]\n",
      "2025-06-01:16:44:58,134 INFO     [lm_eval.api.task:420] Building contexts for mmlu_miscellaneous on rank 0...\n",
      "100%|████████████████████████████████████████| 783/783 [00:00<00:00, 786.55it/s]\n",
      "2025-06-01:16:44:59,160 INFO     [lm_eval.api.task:420] Building contexts for mmlu_management on rank 0...\n",
      "100%|████████████████████████████████████████| 103/103 [00:00<00:00, 779.35it/s]\n",
      "2025-06-01:16:44:59,296 INFO     [lm_eval.api.task:420] Building contexts for mmlu_virology on rank 0...\n",
      "100%|████████████████████████████████████████| 166/166 [00:00<00:00, 788.30it/s]\n",
      "2025-06-01:16:44:59,514 INFO     [lm_eval.api.task:420] Building contexts for mmlu_college_medicine on rank 0...\n",
      "100%|████████████████████████████████████████| 173/173 [00:00<00:00, 787.60it/s]\n",
      "2025-06-01:16:44:59,741 INFO     [lm_eval.api.task:420] Building contexts for mmlu_global_facts on rank 0...\n",
      "100%|████████████████████████████████████████| 100/100 [00:00<00:00, 784.88it/s]\n",
      "2025-06-01:16:44:59,872 INFO     [lm_eval.api.task:420] Building contexts for mmlu_human_aging on rank 0...\n",
      "100%|████████████████████████████████████████| 223/223 [00:00<00:00, 783.21it/s]\n",
      "2025-06-01:16:45:00,166 INFO     [lm_eval.api.task:420] Building contexts for mmlu_high_school_microeconomics on rank 0...\n",
      "100%|████████████████████████████████████████| 238/238 [00:00<00:00, 786.04it/s]\n",
      "2025-06-01:16:45:00,479 INFO     [lm_eval.api.task:420] Building contexts for mmlu_security_studies on rank 0...\n",
      "100%|████████████████████████████████████████| 245/245 [00:00<00:00, 784.83it/s]\n",
      "2025-06-01:16:45:00,801 INFO     [lm_eval.api.task:420] Building contexts for mmlu_econometrics on rank 0...\n",
      "100%|████████████████████████████████████████| 114/114 [00:00<00:00, 783.33it/s]\n",
      "2025-06-01:16:45:00,952 INFO     [lm_eval.api.task:420] Building contexts for mmlu_us_foreign_policy on rank 0...\n",
      "100%|████████████████████████████████████████| 100/100 [00:00<00:00, 784.90it/s]\n",
      "2025-06-01:16:45:01,084 INFO     [lm_eval.api.task:420] Building contexts for mmlu_public_relations on rank 0...\n",
      "100%|████████████████████████████████████████| 110/110 [00:00<00:00, 789.21it/s]\n",
      "2025-06-01:16:45:01,228 INFO     [lm_eval.api.task:420] Building contexts for mmlu_high_school_macroeconomics on rank 0...\n",
      "100%|████████████████████████████████████████| 390/390 [00:00<00:00, 782.28it/s]\n",
      "2025-06-01:16:45:01,742 INFO     [lm_eval.api.task:420] Building contexts for mmlu_high_school_geography on rank 0...\n",
      "100%|████████████████████████████████████████| 198/198 [00:00<00:00, 769.68it/s]\n",
      "2025-06-01:16:45:02,007 INFO     [lm_eval.api.task:420] Building contexts for mmlu_high_school_psychology on rank 0...\n",
      "100%|████████████████████████████████████████| 545/545 [00:00<00:00, 788.58it/s]\n",
      "2025-06-01:16:45:02,720 INFO     [lm_eval.api.task:420] Building contexts for mmlu_human_sexuality on rank 0...\n",
      "100%|████████████████████████████████████████| 131/131 [00:00<00:00, 787.26it/s]\n",
      "2025-06-01:16:45:02,892 INFO     [lm_eval.api.task:420] Building contexts for mmlu_professional_psychology on rank 0...\n",
      "100%|████████████████████████████████████████| 612/612 [00:00<00:00, 777.17it/s]\n",
      "2025-06-01:16:45:03,704 INFO     [lm_eval.api.task:420] Building contexts for mmlu_high_school_government_and_politics on rank 0...\n",
      "100%|████████████████████████████████████████| 193/193 [00:00<00:00, 788.11it/s]\n",
      "2025-06-01:16:45:03,957 INFO     [lm_eval.api.task:420] Building contexts for mmlu_sociology on rank 0...\n",
      "100%|████████████████████████████████████████| 201/201 [00:00<00:00, 792.90it/s]\n",
      "2025-06-01:16:45:04,218 INFO     [lm_eval.api.task:420] Building contexts for mmlu_prehistory on rank 0...\n",
      "100%|████████████████████████████████████████| 324/324 [00:00<00:00, 788.58it/s]\n",
      "2025-06-01:16:45:04,642 INFO     [lm_eval.api.task:420] Building contexts for mmlu_jurisprudence on rank 0...\n",
      "100%|████████████████████████████████████████| 108/108 [00:00<00:00, 789.24it/s]\n",
      "2025-06-01:16:45:04,784 INFO     [lm_eval.api.task:420] Building contexts for mmlu_professional_law on rank 0...\n",
      "100%|██████████████████████████████████████| 1534/1534 [00:02<00:00, 690.32it/s]\n",
      "2025-06-01:16:45:07,067 INFO     [lm_eval.api.task:420] Building contexts for mmlu_high_school_world_history on rank 0...\n",
      "100%|████████████████████████████████████████| 237/237 [00:00<00:00, 779.91it/s]\n",
      "2025-06-01:16:45:07,381 INFO     [lm_eval.api.task:420] Building contexts for mmlu_moral_scenarios on rank 0...\n",
      "100%|████████████████████████████████████████| 895/895 [00:01<00:00, 789.10it/s]\n",
      "2025-06-01:16:45:08,550 INFO     [lm_eval.api.task:420] Building contexts for mmlu_formal_logic on rank 0...\n",
      "100%|████████████████████████████████████████| 126/126 [00:00<00:00, 789.94it/s]\n",
      "2025-06-01:16:45:08,715 INFO     [lm_eval.api.task:420] Building contexts for mmlu_international_law on rank 0...\n",
      "100%|████████████████████████████████████████| 121/121 [00:00<00:00, 784.95it/s]\n",
      "2025-06-01:16:45:08,874 INFO     [lm_eval.api.task:420] Building contexts for mmlu_philosophy on rank 0...\n",
      "100%|████████████████████████████████████████| 311/311 [00:00<00:00, 791.84it/s]\n",
      "2025-06-01:16:45:09,279 INFO     [lm_eval.api.task:420] Building contexts for mmlu_high_school_european_history on rank 0...\n",
      "100%|████████████████████████████████████████| 165/165 [00:00<00:00, 781.08it/s]\n",
      "2025-06-01:16:45:09,498 INFO     [lm_eval.api.task:420] Building contexts for mmlu_world_religions on rank 0...\n",
      "100%|████████████████████████████████████████| 171/171 [00:00<00:00, 779.36it/s]\n",
      "2025-06-01:16:45:09,724 INFO     [lm_eval.api.task:420] Building contexts for mmlu_moral_disputes on rank 0...\n",
      "100%|████████████████████████████████████████| 346/346 [00:00<00:00, 784.04it/s]\n",
      "2025-06-01:16:45:10,179 INFO     [lm_eval.api.task:420] Building contexts for mmlu_high_school_us_history on rank 0...\n",
      "100%|████████████████████████████████████████| 204/204 [00:00<00:00, 782.81it/s]\n",
      "2025-06-01:16:45:10,448 INFO     [lm_eval.api.task:420] Building contexts for mmlu_logical_fallacies on rank 0...\n",
      "100%|████████████████████████████████████████| 163/163 [00:00<00:00, 786.80it/s]\n",
      "2025-06-01:16:45:10,662 INFO     [lm_eval.evaluator:517] Running loglikelihood requests\n",
      "Running loglikelihood requests: 100%|████| 70840/70840 [04:17<00:00, 275.56it/s]\n",
      "2025-06-01:16:50:14,166 INFO     [lm_eval.loggers.evaluation_tracker:209] Saving results aggregated\n",
      "2025-06-01:16:50:14,181 INFO     [lm_eval.loggers.evaluation_tracker:290] Saving per-sample results for: mmlu_abstract_algebra\n",
      "2025-06-01:16:50:14,188 INFO     [lm_eval.loggers.evaluation_tracker:290] Saving per-sample results for: mmlu_anatomy\n",
      "2025-06-01:16:50:14,196 INFO     [lm_eval.loggers.evaluation_tracker:290] Saving per-sample results for: mmlu_astronomy\n",
      "2025-06-01:16:50:14,207 INFO     [lm_eval.loggers.evaluation_tracker:290] Saving per-sample results for: mmlu_business_ethics\n",
      "2025-06-01:16:50:14,214 INFO     [lm_eval.loggers.evaluation_tracker:290] Saving per-sample results for: mmlu_clinical_knowledge\n",
      "2025-06-01:16:50:14,231 INFO     [lm_eval.loggers.evaluation_tracker:290] Saving per-sample results for: mmlu_college_biology\n",
      "2025-06-01:16:50:14,241 INFO     [lm_eval.loggers.evaluation_tracker:290] Saving per-sample results for: mmlu_college_chemistry\n",
      "2025-06-01:16:50:14,249 INFO     [lm_eval.loggers.evaluation_tracker:290] Saving per-sample results for: mmlu_college_computer_science\n",
      "2025-06-01:16:50:14,256 INFO     [lm_eval.loggers.evaluation_tracker:290] Saving per-sample results for: mmlu_college_mathematics\n",
      "2025-06-01:16:50:14,263 INFO     [lm_eval.loggers.evaluation_tracker:290] Saving per-sample results for: mmlu_college_medicine\n",
      "2025-06-01:16:50:14,276 INFO     [lm_eval.loggers.evaluation_tracker:290] Saving per-sample results for: mmlu_college_physics\n",
      "2025-06-01:16:50:14,283 INFO     [lm_eval.loggers.evaluation_tracker:290] Saving per-sample results for: mmlu_computer_security\n",
      "2025-06-01:16:50:14,671 INFO     [lm_eval.loggers.evaluation_tracker:290] Saving per-sample results for: mmlu_conceptual_physics\n",
      "2025-06-01:16:50:14,686 INFO     [lm_eval.loggers.evaluation_tracker:290] Saving per-sample results for: mmlu_econometrics\n",
      "2025-06-01:16:50:14,694 INFO     [lm_eval.loggers.evaluation_tracker:290] Saving per-sample results for: mmlu_electrical_engineering\n",
      "2025-06-01:16:50:14,702 INFO     [lm_eval.loggers.evaluation_tracker:290] Saving per-sample results for: mmlu_elementary_mathematics\n",
      "2025-06-01:16:50:14,725 INFO     [lm_eval.loggers.evaluation_tracker:290] Saving per-sample results for: mmlu_formal_logic\n",
      "2025-06-01:16:50:14,735 INFO     [lm_eval.loggers.evaluation_tracker:290] Saving per-sample results for: mmlu_global_facts\n",
      "2025-06-01:16:50:14,741 INFO     [lm_eval.loggers.evaluation_tracker:290] Saving per-sample results for: mmlu_high_school_biology\n",
      "2025-06-01:16:50:14,763 INFO     [lm_eval.loggers.evaluation_tracker:290] Saving per-sample results for: mmlu_high_school_chemistry\n",
      "2025-06-01:16:50:14,777 INFO     [lm_eval.loggers.evaluation_tracker:290] Saving per-sample results for: mmlu_high_school_computer_science\n",
      "2025-06-01:16:50:14,784 INFO     [lm_eval.loggers.evaluation_tracker:290] Saving per-sample results for: mmlu_high_school_european_history\n",
      "2025-06-01:16:50:14,805 INFO     [lm_eval.loggers.evaluation_tracker:290] Saving per-sample results for: mmlu_high_school_geography\n",
      "2025-06-01:16:50:14,818 INFO     [lm_eval.loggers.evaluation_tracker:290] Saving per-sample results for: mmlu_high_school_government_and_politics\n",
      "2025-06-01:16:50:14,831 INFO     [lm_eval.loggers.evaluation_tracker:290] Saving per-sample results for: mmlu_high_school_macroeconomics\n",
      "2025-06-01:16:50:14,858 INFO     [lm_eval.loggers.evaluation_tracker:290] Saving per-sample results for: mmlu_high_school_mathematics\n",
      "2025-06-01:16:50:14,876 INFO     [lm_eval.loggers.evaluation_tracker:290] Saving per-sample results for: mmlu_high_school_microeconomics\n",
      "2025-06-01:16:50:14,894 INFO     [lm_eval.loggers.evaluation_tracker:290] Saving per-sample results for: mmlu_high_school_physics\n",
      "2025-06-01:16:50:14,905 INFO     [lm_eval.loggers.evaluation_tracker:290] Saving per-sample results for: mmlu_high_school_psychology\n",
      "2025-06-01:16:50:14,943 INFO     [lm_eval.loggers.evaluation_tracker:290] Saving per-sample results for: mmlu_high_school_statistics\n",
      "2025-06-01:16:50:14,959 INFO     [lm_eval.loggers.evaluation_tracker:290] Saving per-sample results for: mmlu_high_school_us_history\n",
      "2025-06-01:16:50:14,986 INFO     [lm_eval.loggers.evaluation_tracker:290] Saving per-sample results for: mmlu_high_school_world_history\n",
      "2025-06-01:16:50:15,016 INFO     [lm_eval.loggers.evaluation_tracker:290] Saving per-sample results for: mmlu_human_aging\n",
      "2025-06-01:16:50:15,030 INFO     [lm_eval.loggers.evaluation_tracker:290] Saving per-sample results for: mmlu_human_sexuality\n",
      "2025-06-01:16:50:15,038 INFO     [lm_eval.loggers.evaluation_tracker:290] Saving per-sample results for: mmlu_international_law\n",
      "2025-06-01:16:50:15,046 INFO     [lm_eval.loggers.evaluation_tracker:290] Saving per-sample results for: mmlu_jurisprudence\n",
      "2025-06-01:16:50:15,054 INFO     [lm_eval.loggers.evaluation_tracker:290] Saving per-sample results for: mmlu_logical_fallacies\n",
      "2025-06-01:16:50:15,066 INFO     [lm_eval.loggers.evaluation_tracker:290] Saving per-sample results for: mmlu_machine_learning\n",
      "2025-06-01:16:50:15,075 INFO     [lm_eval.loggers.evaluation_tracker:290] Saving per-sample results for: mmlu_management\n",
      "2025-06-01:16:50:15,081 INFO     [lm_eval.loggers.evaluation_tracker:290] Saving per-sample results for: mmlu_marketing\n",
      "2025-06-01:16:50:15,098 INFO     [lm_eval.loggers.evaluation_tracker:290] Saving per-sample results for: mmlu_medical_genetics\n",
      "2025-06-01:16:50:15,106 INFO     [lm_eval.loggers.evaluation_tracker:290] Saving per-sample results for: mmlu_miscellaneous\n",
      "2025-06-01:16:50:15,159 INFO     [lm_eval.loggers.evaluation_tracker:290] Saving per-sample results for: mmlu_moral_disputes\n",
      "2025-06-01:16:50:15,183 INFO     [lm_eval.loggers.evaluation_tracker:290] Saving per-sample results for: mmlu_moral_scenarios\n",
      "2025-06-01:16:50:15,249 INFO     [lm_eval.loggers.evaluation_tracker:290] Saving per-sample results for: mmlu_nutrition\n",
      "2025-06-01:16:50:15,272 INFO     [lm_eval.loggers.evaluation_tracker:290] Saving per-sample results for: mmlu_philosophy\n",
      "2025-06-01:16:50:15,294 INFO     [lm_eval.loggers.evaluation_tracker:290] Saving per-sample results for: mmlu_prehistory\n",
      "2025-06-01:16:50:15,317 INFO     [lm_eval.loggers.evaluation_tracker:290] Saving per-sample results for: mmlu_professional_accounting\n",
      "2025-06-01:16:50:15,340 INFO     [lm_eval.loggers.evaluation_tracker:290] Saving per-sample results for: mmlu_professional_law\n",
      "2025-06-01:16:50:15,496 INFO     [lm_eval.loggers.evaluation_tracker:290] Saving per-sample results for: mmlu_professional_medicine\n",
      "2025-06-01:16:50:15,519 INFO     [lm_eval.loggers.evaluation_tracker:290] Saving per-sample results for: mmlu_professional_psychology\n",
      "2025-06-01:16:50:15,565 INFO     [lm_eval.loggers.evaluation_tracker:290] Saving per-sample results for: mmlu_public_relations\n",
      "2025-06-01:16:50:15,572 INFO     [lm_eval.loggers.evaluation_tracker:290] Saving per-sample results for: mmlu_security_studies\n",
      "2025-06-01:16:50:15,593 INFO     [lm_eval.loggers.evaluation_tracker:290] Saving per-sample results for: mmlu_sociology\n",
      "2025-06-01:16:50:15,609 INFO     [lm_eval.loggers.evaluation_tracker:290] Saving per-sample results for: mmlu_us_foreign_policy\n",
      "2025-06-01:16:50:15,615 INFO     [lm_eval.loggers.evaluation_tracker:290] Saving per-sample results for: mmlu_virology\n",
      "2025-06-01:16:50:15,626 INFO     [lm_eval.loggers.evaluation_tracker:290] Saving per-sample results for: mmlu_world_religions\n",
      "2025-06-01:16:50:15,636 INFO     [lm_eval.loggers.evaluation_tracker:290] Saving per-sample results for: wmdp_bio\n",
      "2025-06-01:16:50:15,723 INFO     [lm_eval.loggers.evaluation_tracker:290] Saving per-sample results for: wmdp_chem\n",
      "2025-06-01:16:50:15,752 INFO     [lm_eval.loggers.evaluation_tracker:290] Saving per-sample results for: wmdp_cyber\n",
      "hf (pretrained=../rmu_results/layer9_alpha300.0_steer350.0,dtype=bfloat16), gen_kwargs: (None), limit: None, num_fewshot: None, batch_size: 8\n",
      "|                 Tasks                 |Version|Filter|n-shot|Metric|   |Value |   |Stderr|\n",
      "|---------------------------------------|------:|------|-----:|------|---|-----:|---|-----:|\n",
      "|mmlu                                   |      2|none  |      |acc   |↑  |0.4335|±  |0.0040|\n",
      "| - humanities                          |      2|none  |      |acc   |↑  |0.4193|±  |0.0069|\n",
      "|  - formal_logic                       |      1|none  |     0|acc   |↑  |0.3254|±  |0.0419|\n",
      "|  - high_school_european_history       |      1|none  |     0|acc   |↑  |0.5455|±  |0.0389|\n",
      "|  - high_school_us_history             |      1|none  |     0|acc   |↑  |0.5931|±  |0.0345|\n",
      "|  - high_school_world_history          |      1|none  |     0|acc   |↑  |0.5865|±  |0.0321|\n",
      "|  - international_law                  |      1|none  |     0|acc   |↑  |0.6612|±  |0.0432|\n",
      "|  - jurisprudence                      |      1|none  |     0|acc   |↑  |0.5093|±  |0.0483|\n",
      "|  - logical_fallacies                  |      1|none  |     0|acc   |↑  |0.5031|±  |0.0393|\n",
      "|  - moral_disputes                     |      1|none  |     0|acc   |↑  |0.4827|±  |0.0269|\n",
      "|  - moral_scenarios                    |      1|none  |     0|acc   |↑  |0.2324|±  |0.0141|\n",
      "|  - philosophy                         |      1|none  |     0|acc   |↑  |0.5273|±  |0.0284|\n",
      "|  - prehistory                         |      1|none  |     0|acc   |↑  |0.5463|±  |0.0277|\n",
      "|  - professional_law                   |      1|none  |     0|acc   |↑  |0.3553|±  |0.0122|\n",
      "|  - world_religions                    |      1|none  |     0|acc   |↑  |0.6082|±  |0.0374|\n",
      "| - other                               |      2|none  |      |acc   |↑  |0.4483|±  |0.0085|\n",
      "|  - business_ethics                    |      1|none  |     0|acc   |↑  |0.4200|±  |0.0496|\n",
      "|  - clinical_knowledge                 |      1|none  |     0|acc   |↑  |0.3811|±  |0.0299|\n",
      "|  - college_medicine                   |      1|none  |     0|acc   |↑  |0.3873|±  |0.0371|\n",
      "|  - global_facts                       |      1|none  |     0|acc   |↑  |0.2700|±  |0.0446|\n",
      "|  - human_aging                        |      1|none  |     0|acc   |↑  |0.4978|±  |0.0336|\n",
      "|  - management                         |      1|none  |     0|acc   |↑  |0.6408|±  |0.0475|\n",
      "|  - marketing                          |      1|none  |     0|acc   |↑  |0.7137|±  |0.0296|\n",
      "|  - medical_genetics                   |      1|none  |     0|acc   |↑  |0.4200|±  |0.0496|\n",
      "|  - miscellaneous                      |      1|none  |     0|acc   |↑  |0.5913|±  |0.0176|\n",
      "|  - nutrition                          |      1|none  |     0|acc   |↑  |0.3137|±  |0.0266|\n",
      "|  - professional_accounting            |      1|none  |     0|acc   |↑  |0.3440|±  |0.0283|\n",
      "|  - professional_medicine              |      1|none  |     0|acc   |↑  |0.2279|±  |0.0255|\n",
      "|  - virology                           |      1|none  |     0|acc   |↑  |0.3133|±  |0.0361|\n",
      "| - social sciences                     |      2|none  |      |acc   |↑  |0.5102|±  |0.0088|\n",
      "|  - econometrics                       |      1|none  |     0|acc   |↑  |0.3158|±  |0.0437|\n",
      "|  - high_school_geography              |      1|none  |     0|acc   |↑  |0.5707|±  |0.0353|\n",
      "|  - high_school_government_and_politics|      1|none  |     0|acc   |↑  |0.6269|±  |0.0349|\n",
      "|  - high_school_macroeconomics         |      1|none  |     0|acc   |↑  |0.3949|±  |0.0248|\n",
      "|  - high_school_microeconomics         |      1|none  |     0|acc   |↑  |0.4664|±  |0.0324|\n",
      "|  - high_school_psychology             |      1|none  |     0|acc   |↑  |0.6055|±  |0.0210|\n",
      "|  - human_sexuality                    |      1|none  |     0|acc   |↑  |0.5344|±  |0.0437|\n",
      "|  - professional_psychology            |      1|none  |     0|acc   |↑  |0.4297|±  |0.0200|\n",
      "|  - public_relations                   |      1|none  |     0|acc   |↑  |0.5273|±  |0.0478|\n",
      "|  - security_studies                   |      1|none  |     0|acc   |↑  |0.4245|±  |0.0316|\n",
      "|  - sociology                          |      1|none  |     0|acc   |↑  |0.6965|±  |0.0325|\n",
      "|  - us_foreign_policy                  |      1|none  |     0|acc   |↑  |0.7000|±  |0.0461|\n",
      "| - stem                                |      2|none  |      |acc   |↑  |0.3650|±  |0.0084|\n",
      "|  - abstract_algebra                   |      1|none  |     0|acc   |↑  |0.3200|±  |0.0469|\n",
      "|  - anatomy                            |      1|none  |     0|acc   |↑  |0.4667|±  |0.0431|\n",
      "|  - astronomy                          |      1|none  |     0|acc   |↑  |0.5000|±  |0.0407|\n",
      "|  - college_biology                    |      1|none  |     0|acc   |↑  |0.4931|±  |0.0418|\n",
      "|  - college_chemistry                  |      1|none  |     0|acc   |↑  |0.3500|±  |0.0479|\n",
      "|  - college_computer_science           |      1|none  |     0|acc   |↑  |0.2600|±  |0.0441|\n",
      "|  - college_mathematics                |      1|none  |     0|acc   |↑  |0.2200|±  |0.0416|\n",
      "|  - college_physics                    |      1|none  |     0|acc   |↑  |0.2941|±  |0.0453|\n",
      "|  - computer_security                  |      1|none  |     0|acc   |↑  |0.3000|±  |0.0461|\n",
      "|  - conceptual_physics                 |      1|none  |     0|acc   |↑  |0.4043|±  |0.0321|\n",
      "|  - electrical_engineering             |      1|none  |     0|acc   |↑  |0.4966|±  |0.0417|\n",
      "|  - elementary_mathematics             |      1|none  |     0|acc   |↑  |0.2989|±  |0.0236|\n",
      "|  - high_school_biology                |      1|none  |     0|acc   |↑  |0.5065|±  |0.0284|\n",
      "|  - high_school_chemistry              |      1|none  |     0|acc   |↑  |0.3448|±  |0.0334|\n",
      "|  - high_school_computer_science       |      1|none  |     0|acc   |↑  |0.3000|±  |0.0461|\n",
      "|  - high_school_mathematics            |      1|none  |     0|acc   |↑  |0.2926|±  |0.0277|\n",
      "|  - high_school_physics                |      1|none  |     0|acc   |↑  |0.3709|±  |0.0394|\n",
      "|  - high_school_statistics             |      1|none  |     0|acc   |↑  |0.2824|±  |0.0307|\n",
      "|  - machine_learning                   |      1|none  |     0|acc   |↑  |0.2946|±  |0.0433|\n",
      "|wmdp                                   |      1|none  |      |acc   |↑  |0.2639|±  |0.0073|\n",
      "| - wmdp_bio                            |      1|none  |     0|acc   |↑  |0.2561|±  |0.0122|\n",
      "| - wmdp_chem                           |      1|none  |     0|acc   |↑  |0.2696|±  |0.0220|\n",
      "| - wmdp_cyber                          |      1|none  |     0|acc   |↑  |0.2677|±  |0.0099|\n",
      "\n",
      "|      Groups      |Version|Filter|n-shot|Metric|   |Value |   |Stderr|\n",
      "|------------------|------:|------|------|------|---|-----:|---|-----:|\n",
      "|mmlu              |      2|none  |      |acc   |↑  |0.4335|±  |0.0040|\n",
      "| - humanities     |      2|none  |      |acc   |↑  |0.4193|±  |0.0069|\n",
      "| - other          |      2|none  |      |acc   |↑  |0.4483|±  |0.0085|\n",
      "| - social sciences|      2|none  |      |acc   |↑  |0.5102|±  |0.0088|\n",
      "| - stem           |      2|none  |      |acc   |↑  |0.3650|±  |0.0084|\n",
      "|wmdp              |      1|none  |      |acc   |↑  |0.2639|±  |0.0073|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!uv run lm-eval --model hf \\\n",
    "    --model_args pretrained=\"../rmu_results/layer9_alpha300.0_steer350.0\",dtype=bfloat16 \\\n",
    "    --tasks mmlu,wmdp \\\n",
    "    --log_samples \\\n",
    "    --output_path \"eval/gemma-2b\" \\\n",
    "    --batch_size=8 \\\n",
    "    --seed 42"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
