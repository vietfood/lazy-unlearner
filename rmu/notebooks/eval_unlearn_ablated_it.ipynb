{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "54b787d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-07-04:07:17:56,996 INFO     [lm_eval.__main__:379] Selected Tasks: ['mmlu', 'wmdp']\n",
      "2025-07-04:07:17:56,997 INFO     [lm_eval.evaluator:169] Setting random seed to 42 | Setting numpy seed to 42 | Setting torch manual seed to 42 | Setting fewshot manual seed to 42\n",
      "2025-07-04:07:17:56,998 INFO     [lm_eval.evaluator:206] Initializing hf model, with arguments: {'pretrained': '/home/ubuntu/thesis/sae/results/models/sae_steer', 'dtype': 'bfloat16'}\n",
      "2025-07-04:07:17:57,456 INFO     [lm_eval.models.huggingface:136] Using device 'cuda'\n",
      "2025-07-04:07:17:58,479 INFO     [lm_eval.models.huggingface:376] Model parallel was set to False, max memory was not set, and device map was set to {'': 'cuda'}\n",
      "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:01<00:00,  1.15it/s]\n",
      "2025-07-04:07:18:00,424 INFO     [lm_eval.models.huggingface:223] Model type is 'gemma2', part of the Gemma family--a BOS token will be used as Gemma underperforms without it.\n",
      "2025-07-04:07:18:21,356 WARNING  [lm_eval.api.task:327] [Task: wmdp_cyber] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.\n",
      "2025-07-04:07:18:21,356 WARNING  [lm_eval.api.task:327] [Task: wmdp_cyber] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.\n",
      "2025-07-04:07:18:21,993 WARNING  [lm_eval.api.task:327] [Task: wmdp_chem] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.\n",
      "2025-07-04:07:18:21,993 WARNING  [lm_eval.api.task:327] [Task: wmdp_chem] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.\n",
      "2025-07-04:07:18:22,555 WARNING  [lm_eval.api.task:327] [Task: wmdp_bio] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.\n",
      "2025-07-04:07:18:22,555 WARNING  [lm_eval.api.task:327] [Task: wmdp_bio] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.\n",
      "2025-07-04:07:18:22,603 INFO     [lm_eval.api.task:420] Building contexts for wmdp_bio on rank 0...\n",
      "100%|██████████████████████████████████████| 1273/1273 [00:01<00:00, 753.57it/s]\n",
      "2025-07-04:07:18:24,335 INFO     [lm_eval.api.task:420] Building contexts for wmdp_chem on rank 0...\n",
      "100%|████████████████████████████████████████| 408/408 [00:00<00:00, 749.10it/s]\n",
      "2025-07-04:07:18:24,895 INFO     [lm_eval.api.task:420] Building contexts for wmdp_cyber on rank 0...\n",
      "100%|██████████████████████████████████████| 1987/1987 [00:02<00:00, 681.15it/s]\n",
      "2025-07-04:07:18:27,892 INFO     [lm_eval.api.task:420] Building contexts for mmlu_high_school_computer_science on rank 0...\n",
      "100%|████████████████████████████████████████| 100/100 [00:00<00:00, 769.83it/s]\n",
      "2025-07-04:07:18:28,026 INFO     [lm_eval.api.task:420] Building contexts for mmlu_high_school_chemistry on rank 0...\n",
      "100%|████████████████████████████████████████| 203/203 [00:00<00:00, 737.69it/s]\n",
      "2025-07-04:07:18:28,310 INFO     [lm_eval.api.task:420] Building contexts for mmlu_abstract_algebra on rank 0...\n",
      "100%|████████████████████████████████████████| 100/100 [00:00<00:00, 774.35it/s]\n",
      "2025-07-04:07:18:28,443 INFO     [lm_eval.api.task:420] Building contexts for mmlu_machine_learning on rank 0...\n",
      "100%|████████████████████████████████████████| 112/112 [00:00<00:00, 778.16it/s]\n",
      "2025-07-04:07:18:28,592 INFO     [lm_eval.api.task:420] Building contexts for mmlu_astronomy on rank 0...\n",
      "100%|████████████████████████████████████████| 152/152 [00:00<00:00, 722.95it/s]\n",
      "2025-07-04:07:18:28,809 INFO     [lm_eval.api.task:420] Building contexts for mmlu_elementary_mathematics on rank 0...\n",
      "100%|████████████████████████████████████████| 378/378 [00:00<00:00, 752.46it/s]\n",
      "2025-07-04:07:18:29,326 INFO     [lm_eval.api.task:420] Building contexts for mmlu_conceptual_physics on rank 0...\n",
      "100%|████████████████████████████████████████| 235/235 [00:00<00:00, 779.10it/s]\n",
      "2025-07-04:07:18:29,637 INFO     [lm_eval.api.task:420] Building contexts for mmlu_college_computer_science on rank 0...\n",
      "100%|████████████████████████████████████████| 100/100 [00:00<00:00, 702.18it/s]\n",
      "2025-07-04:07:18:29,784 INFO     [lm_eval.api.task:420] Building contexts for mmlu_high_school_biology on rank 0...\n",
      "100%|████████████████████████████████████████| 310/310 [00:00<00:00, 773.01it/s]\n",
      "2025-07-04:07:18:30,198 INFO     [lm_eval.api.task:420] Building contexts for mmlu_high_school_mathematics on rank 0...\n",
      "100%|████████████████████████████████████████| 270/270 [00:00<00:00, 751.67it/s]\n",
      "2025-07-04:07:18:30,570 INFO     [lm_eval.api.task:420] Building contexts for mmlu_college_chemistry on rank 0...\n",
      "100%|████████████████████████████████████████| 100/100 [00:00<00:00, 775.44it/s]\n",
      "2025-07-04:07:18:30,704 INFO     [lm_eval.api.task:420] Building contexts for mmlu_college_physics on rank 0...\n",
      "100%|████████████████████████████████████████| 102/102 [00:00<00:00, 707.11it/s]\n",
      "2025-07-04:07:18:30,852 INFO     [lm_eval.api.task:420] Building contexts for mmlu_computer_security on rank 0...\n",
      "100%|████████████████████████████████████████| 100/100 [00:00<00:00, 773.95it/s]\n",
      "2025-07-04:07:18:30,986 INFO     [lm_eval.api.task:420] Building contexts for mmlu_anatomy on rank 0...\n",
      "100%|████████████████████████████████████████| 135/135 [00:00<00:00, 777.20it/s]\n",
      "2025-07-04:07:18:31,166 INFO     [lm_eval.api.task:420] Building contexts for mmlu_high_school_statistics on rank 0...\n",
      "100%|████████████████████████████████████████| 216/216 [00:00<00:00, 737.24it/s]\n",
      "2025-07-04:07:18:31,468 INFO     [lm_eval.api.task:420] Building contexts for mmlu_college_biology on rank 0...\n",
      "100%|████████████████████████████████████████| 144/144 [00:00<00:00, 775.88it/s]\n",
      "2025-07-04:07:18:31,659 INFO     [lm_eval.api.task:420] Building contexts for mmlu_high_school_physics on rank 0...\n",
      "100%|████████████████████████████████████████| 151/151 [00:00<00:00, 729.60it/s]\n",
      "2025-07-04:07:18:31,873 INFO     [lm_eval.api.task:420] Building contexts for mmlu_electrical_engineering on rank 0...\n",
      "100%|████████████████████████████████████████| 145/145 [00:00<00:00, 777.85it/s]\n",
      "2025-07-04:07:18:32,065 INFO     [lm_eval.api.task:420] Building contexts for mmlu_college_mathematics on rank 0...\n",
      "100%|████████████████████████████████████████| 100/100 [00:00<00:00, 772.41it/s]\n",
      "2025-07-04:07:18:32,199 INFO     [lm_eval.api.task:420] Building contexts for mmlu_clinical_knowledge on rank 0...\n",
      "100%|████████████████████████████████████████| 265/265 [00:00<00:00, 744.32it/s]\n",
      "2025-07-04:07:18:32,566 INFO     [lm_eval.api.task:420] Building contexts for mmlu_professional_medicine on rank 0...\n",
      "100%|████████████████████████████████████████| 272/272 [00:00<00:00, 726.60it/s]\n",
      "2025-07-04:07:18:32,952 INFO     [lm_eval.api.task:420] Building contexts for mmlu_nutrition on rank 0...\n",
      "100%|████████████████████████████████████████| 306/306 [00:00<00:00, 779.88it/s]\n",
      "2025-07-04:07:18:33,357 INFO     [lm_eval.api.task:420] Building contexts for mmlu_virology on rank 0...\n",
      "100%|████████████████████████████████████████| 166/166 [00:00<00:00, 724.07it/s]\n",
      "2025-07-04:07:18:33,593 INFO     [lm_eval.api.task:420] Building contexts for mmlu_business_ethics on rank 0...\n",
      "100%|████████████████████████████████████████| 100/100 [00:00<00:00, 765.35it/s]\n",
      "2025-07-04:07:18:33,729 INFO     [lm_eval.api.task:420] Building contexts for mmlu_miscellaneous on rank 0...\n",
      "100%|████████████████████████████████████████| 783/783 [00:01<00:00, 751.60it/s]\n",
      "2025-07-04:07:18:34,801 INFO     [lm_eval.api.task:420] Building contexts for mmlu_medical_genetics on rank 0...\n",
      "100%|████████████████████████████████████████| 100/100 [00:00<00:00, 768.95it/s]\n",
      "2025-07-04:07:18:34,935 INFO     [lm_eval.api.task:420] Building contexts for mmlu_global_facts on rank 0...\n",
      "100%|████████████████████████████████████████| 100/100 [00:00<00:00, 703.13it/s]\n",
      "2025-07-04:07:18:35,082 INFO     [lm_eval.api.task:420] Building contexts for mmlu_professional_accounting on rank 0...\n",
      "100%|████████████████████████████████████████| 282/282 [00:00<00:00, 769.50it/s]\n",
      "2025-07-04:07:18:35,460 INFO     [lm_eval.api.task:420] Building contexts for mmlu_marketing on rank 0...\n",
      "100%|████████████████████████████████████████| 234/234 [00:00<00:00, 746.96it/s]\n",
      "2025-07-04:07:18:35,783 INFO     [lm_eval.api.task:420] Building contexts for mmlu_human_aging on rank 0...\n",
      "100%|████████████████████████████████████████| 223/223 [00:00<00:00, 755.72it/s]\n",
      "2025-07-04:07:18:36,087 INFO     [lm_eval.api.task:420] Building contexts for mmlu_management on rank 0...\n",
      "100%|████████████████████████████████████████| 103/103 [00:00<00:00, 748.68it/s]\n",
      "2025-07-04:07:18:36,229 INFO     [lm_eval.api.task:420] Building contexts for mmlu_college_medicine on rank 0...\n",
      "100%|████████████████████████████████████████| 173/173 [00:00<00:00, 772.35it/s]\n",
      "2025-07-04:07:18:36,461 INFO     [lm_eval.api.task:420] Building contexts for mmlu_high_school_psychology on rank 0...\n",
      "100%|████████████████████████████████████████| 545/545 [00:00<00:00, 744.82it/s]\n",
      "2025-07-04:07:18:37,214 INFO     [lm_eval.api.task:420] Building contexts for mmlu_high_school_geography on rank 0...\n",
      "100%|████████████████████████████████████████| 198/198 [00:00<00:00, 771.24it/s]\n",
      "2025-07-04:07:18:37,479 INFO     [lm_eval.api.task:420] Building contexts for mmlu_human_sexuality on rank 0...\n",
      "100%|████████████████████████████████████████| 131/131 [00:00<00:00, 739.36it/s]\n",
      "2025-07-04:07:18:37,662 INFO     [lm_eval.api.task:420] Building contexts for mmlu_high_school_microeconomics on rank 0...\n",
      "100%|████████████████████████████████████████| 238/238 [00:00<00:00, 741.00it/s]\n",
      "2025-07-04:07:18:37,997 INFO     [lm_eval.api.task:420] Building contexts for mmlu_sociology on rank 0...\n",
      "100%|████████████████████████████████████████| 201/201 [00:00<00:00, 329.44it/s]\n",
      "2025-07-04:07:18:38,616 INFO     [lm_eval.api.task:420] Building contexts for mmlu_security_studies on rank 0...\n",
      "100%|████████████████████████████████████████| 245/245 [00:00<00:00, 746.40it/s]\n",
      "2025-07-04:07:18:38,954 INFO     [lm_eval.api.task:420] Building contexts for mmlu_professional_psychology on rank 0...\n",
      "100%|████████████████████████████████████████| 612/612 [00:00<00:00, 756.45it/s]\n",
      "2025-07-04:07:18:39,789 INFO     [lm_eval.api.task:420] Building contexts for mmlu_public_relations on rank 0...\n",
      "100%|████████████████████████████████████████| 110/110 [00:00<00:00, 747.41it/s]\n",
      "2025-07-04:07:18:39,943 INFO     [lm_eval.api.task:420] Building contexts for mmlu_econometrics on rank 0...\n",
      "100%|████████████████████████████████████████| 114/114 [00:00<00:00, 771.08it/s]\n",
      "2025-07-04:07:18:40,096 INFO     [lm_eval.api.task:420] Building contexts for mmlu_us_foreign_policy on rank 0...\n",
      "100%|████████████████████████████████████████| 100/100 [00:00<00:00, 764.65it/s]\n",
      "2025-07-04:07:18:40,231 INFO     [lm_eval.api.task:420] Building contexts for mmlu_high_school_macroeconomics on rank 0...\n",
      "100%|████████████████████████████████████████| 390/390 [00:00<00:00, 742.36it/s]\n",
      "2025-07-04:07:18:40,772 INFO     [lm_eval.api.task:420] Building contexts for mmlu_high_school_government_and_politics on rank 0...\n",
      "100%|████████████████████████████████████████| 193/193 [00:00<00:00, 727.24it/s]\n",
      "2025-07-04:07:18:41,046 INFO     [lm_eval.api.task:420] Building contexts for mmlu_professional_law on rank 0...\n",
      "100%|██████████████████████████████████████| 1534/1534 [00:02<00:00, 742.66it/s]\n",
      "2025-07-04:07:18:43,175 INFO     [lm_eval.api.task:420] Building contexts for mmlu_high_school_us_history on rank 0...\n",
      "100%|████████████████████████████████████████| 204/204 [00:00<00:00, 763.92it/s]\n",
      "2025-07-04:07:18:43,452 INFO     [lm_eval.api.task:420] Building contexts for mmlu_moral_disputes on rank 0...\n",
      "100%|████████████████████████████████████████| 346/346 [00:00<00:00, 698.53it/s]\n",
      "2025-07-04:07:18:43,961 INFO     [lm_eval.api.task:420] Building contexts for mmlu_moral_scenarios on rank 0...\n",
      "100%|████████████████████████████████████████| 895/895 [00:01<00:00, 739.95it/s]\n",
      "2025-07-04:07:18:45,208 INFO     [lm_eval.api.task:420] Building contexts for mmlu_logical_fallacies on rank 0...\n",
      "100%|████████████████████████████████████████| 163/163 [00:00<00:00, 764.87it/s]\n",
      "2025-07-04:07:18:45,428 INFO     [lm_eval.api.task:420] Building contexts for mmlu_high_school_world_history on rank 0...\n",
      "100%|████████████████████████████████████████| 237/237 [00:00<00:00, 731.87it/s]\n",
      "2025-07-04:07:18:45,763 INFO     [lm_eval.api.task:420] Building contexts for mmlu_prehistory on rank 0...\n",
      "100%|████████████████████████████████████████| 324/324 [00:00<00:00, 764.86it/s]\n",
      "2025-07-04:07:18:46,201 INFO     [lm_eval.api.task:420] Building contexts for mmlu_jurisprudence on rank 0...\n",
      "100%|████████████████████████████████████████| 108/108 [00:00<00:00, 709.88it/s]\n",
      "2025-07-04:07:18:46,358 INFO     [lm_eval.api.task:420] Building contexts for mmlu_philosophy on rank 0...\n",
      "100%|████████████████████████████████████████| 311/311 [00:00<00:00, 750.24it/s]\n",
      "2025-07-04:07:18:46,785 INFO     [lm_eval.api.task:420] Building contexts for mmlu_world_religions on rank 0...\n",
      "100%|████████████████████████████████████████| 171/171 [00:00<00:00, 759.66it/s]\n",
      "2025-07-04:07:18:47,020 INFO     [lm_eval.api.task:420] Building contexts for mmlu_formal_logic on rank 0...\n",
      "100%|████████████████████████████████████████| 126/126 [00:00<00:00, 768.12it/s]\n",
      "2025-07-04:07:18:47,190 INFO     [lm_eval.api.task:420] Building contexts for mmlu_international_law on rank 0...\n",
      "100%|████████████████████████████████████████| 121/121 [00:00<00:00, 699.92it/s]\n",
      "2025-07-04:07:18:47,368 INFO     [lm_eval.api.task:420] Building contexts for mmlu_high_school_european_history on rank 0...\n",
      "100%|████████████████████████████████████████| 165/165 [00:00<00:00, 746.20it/s]\n",
      "2025-07-04:07:18:47,598 INFO     [lm_eval.evaluator:517] Running loglikelihood requests\n",
      "Running loglikelihood requests: 100%|████| 70840/70840 [04:14<00:00, 278.85it/s]\n",
      "2025-07-04:07:23:52,541 INFO     [lm_eval.loggers.evaluation_tracker:209] Saving results aggregated\n",
      "2025-07-04:07:23:52,565 INFO     [lm_eval.loggers.evaluation_tracker:290] Saving per-sample results for: mmlu_abstract_algebra\n",
      "2025-07-04:07:23:52,572 INFO     [lm_eval.loggers.evaluation_tracker:290] Saving per-sample results for: mmlu_anatomy\n",
      "2025-07-04:07:23:52,581 INFO     [lm_eval.loggers.evaluation_tracker:290] Saving per-sample results for: mmlu_astronomy\n",
      "2025-07-04:07:23:52,592 INFO     [lm_eval.loggers.evaluation_tracker:290] Saving per-sample results for: mmlu_business_ethics\n",
      "2025-07-04:07:23:52,599 INFO     [lm_eval.loggers.evaluation_tracker:290] Saving per-sample results for: mmlu_clinical_knowledge\n",
      "2025-07-04:07:23:52,616 INFO     [lm_eval.loggers.evaluation_tracker:290] Saving per-sample results for: mmlu_college_biology\n",
      "2025-07-04:07:23:52,626 INFO     [lm_eval.loggers.evaluation_tracker:290] Saving per-sample results for: mmlu_college_chemistry\n",
      "2025-07-04:07:23:52,633 INFO     [lm_eval.loggers.evaluation_tracker:290] Saving per-sample results for: mmlu_college_computer_science\n",
      "2025-07-04:07:23:52,641 INFO     [lm_eval.loggers.evaluation_tracker:290] Saving per-sample results for: mmlu_college_mathematics\n",
      "2025-07-04:07:23:52,648 INFO     [lm_eval.loggers.evaluation_tracker:290] Saving per-sample results for: mmlu_college_medicine\n",
      "2025-07-04:07:23:53,276 INFO     [lm_eval.loggers.evaluation_tracker:290] Saving per-sample results for: mmlu_college_physics\n",
      "2025-07-04:07:23:53,283 INFO     [lm_eval.loggers.evaluation_tracker:290] Saving per-sample results for: mmlu_computer_security\n",
      "2025-07-04:07:23:53,290 INFO     [lm_eval.loggers.evaluation_tracker:290] Saving per-sample results for: mmlu_conceptual_physics\n",
      "2025-07-04:07:23:53,305 INFO     [lm_eval.loggers.evaluation_tracker:290] Saving per-sample results for: mmlu_econometrics\n",
      "2025-07-04:07:23:53,313 INFO     [lm_eval.loggers.evaluation_tracker:290] Saving per-sample results for: mmlu_electrical_engineering\n",
      "2025-07-04:07:23:53,321 INFO     [lm_eval.loggers.evaluation_tracker:290] Saving per-sample results for: mmlu_elementary_mathematics\n",
      "2025-07-04:07:23:53,345 INFO     [lm_eval.loggers.evaluation_tracker:290] Saving per-sample results for: mmlu_formal_logic\n",
      "2025-07-04:07:23:53,355 INFO     [lm_eval.loggers.evaluation_tracker:290] Saving per-sample results for: mmlu_global_facts\n",
      "2025-07-04:07:23:53,362 INFO     [lm_eval.loggers.evaluation_tracker:290] Saving per-sample results for: mmlu_high_school_biology\n",
      "2025-07-04:07:23:53,384 INFO     [lm_eval.loggers.evaluation_tracker:290] Saving per-sample results for: mmlu_high_school_chemistry\n",
      "2025-07-04:07:23:53,399 INFO     [lm_eval.loggers.evaluation_tracker:290] Saving per-sample results for: mmlu_high_school_computer_science\n",
      "2025-07-04:07:23:53,407 INFO     [lm_eval.loggers.evaluation_tracker:290] Saving per-sample results for: mmlu_high_school_european_history\n",
      "2025-07-04:07:23:53,433 INFO     [lm_eval.loggers.evaluation_tracker:290] Saving per-sample results for: mmlu_high_school_geography\n",
      "2025-07-04:07:23:53,446 INFO     [lm_eval.loggers.evaluation_tracker:290] Saving per-sample results for: mmlu_high_school_government_and_politics\n",
      "2025-07-04:07:23:53,459 INFO     [lm_eval.loggers.evaluation_tracker:290] Saving per-sample results for: mmlu_high_school_macroeconomics\n",
      "2025-07-04:07:23:53,484 INFO     [lm_eval.loggers.evaluation_tracker:290] Saving per-sample results for: mmlu_high_school_mathematics\n",
      "2025-07-04:07:23:53,503 INFO     [lm_eval.loggers.evaluation_tracker:290] Saving per-sample results for: mmlu_high_school_microeconomics\n",
      "2025-07-04:07:23:53,521 INFO     [lm_eval.loggers.evaluation_tracker:290] Saving per-sample results for: mmlu_high_school_physics\n",
      "2025-07-04:07:23:53,533 INFO     [lm_eval.loggers.evaluation_tracker:290] Saving per-sample results for: mmlu_high_school_psychology\n",
      "2025-07-04:07:23:53,575 INFO     [lm_eval.loggers.evaluation_tracker:290] Saving per-sample results for: mmlu_high_school_statistics\n",
      "2025-07-04:07:23:53,591 INFO     [lm_eval.loggers.evaluation_tracker:290] Saving per-sample results for: mmlu_high_school_us_history\n",
      "2025-07-04:07:23:53,620 INFO     [lm_eval.loggers.evaluation_tracker:290] Saving per-sample results for: mmlu_high_school_world_history\n",
      "2025-07-04:07:23:53,654 INFO     [lm_eval.loggers.evaluation_tracker:290] Saving per-sample results for: mmlu_human_aging\n",
      "2025-07-04:07:23:53,668 INFO     [lm_eval.loggers.evaluation_tracker:290] Saving per-sample results for: mmlu_human_sexuality\n",
      "2025-07-04:07:23:53,676 INFO     [lm_eval.loggers.evaluation_tracker:290] Saving per-sample results for: mmlu_international_law\n",
      "2025-07-04:07:23:53,686 INFO     [lm_eval.loggers.evaluation_tracker:290] Saving per-sample results for: mmlu_jurisprudence\n",
      "2025-07-04:07:23:53,694 INFO     [lm_eval.loggers.evaluation_tracker:290] Saving per-sample results for: mmlu_logical_fallacies\n",
      "2025-07-04:07:23:53,705 INFO     [lm_eval.loggers.evaluation_tracker:290] Saving per-sample results for: mmlu_machine_learning\n",
      "2025-07-04:07:23:53,713 INFO     [lm_eval.loggers.evaluation_tracker:290] Saving per-sample results for: mmlu_management\n",
      "2025-07-04:07:23:53,724 INFO     [lm_eval.loggers.evaluation_tracker:290] Saving per-sample results for: mmlu_marketing\n",
      "2025-07-04:07:23:53,738 INFO     [lm_eval.loggers.evaluation_tracker:290] Saving per-sample results for: mmlu_medical_genetics\n",
      "2025-07-04:07:23:53,745 INFO     [lm_eval.loggers.evaluation_tracker:290] Saving per-sample results for: mmlu_miscellaneous\n",
      "2025-07-04:07:23:53,793 INFO     [lm_eval.loggers.evaluation_tracker:290] Saving per-sample results for: mmlu_moral_disputes\n",
      "2025-07-04:07:23:53,816 INFO     [lm_eval.loggers.evaluation_tracker:290] Saving per-sample results for: mmlu_moral_scenarios\n",
      "2025-07-04:07:23:53,879 INFO     [lm_eval.loggers.evaluation_tracker:290] Saving per-sample results for: mmlu_nutrition\n",
      "2025-07-04:07:23:53,900 INFO     [lm_eval.loggers.evaluation_tracker:290] Saving per-sample results for: mmlu_philosophy\n",
      "2025-07-04:07:23:53,921 INFO     [lm_eval.loggers.evaluation_tracker:290] Saving per-sample results for: mmlu_prehistory\n",
      "2025-07-04:07:23:53,943 INFO     [lm_eval.loggers.evaluation_tracker:290] Saving per-sample results for: mmlu_professional_accounting\n",
      "2025-07-04:07:23:53,966 INFO     [lm_eval.loggers.evaluation_tracker:290] Saving per-sample results for: mmlu_professional_law\n",
      "2025-07-04:07:23:54,141 INFO     [lm_eval.loggers.evaluation_tracker:290] Saving per-sample results for: mmlu_professional_medicine\n",
      "2025-07-04:07:23:54,167 INFO     [lm_eval.loggers.evaluation_tracker:290] Saving per-sample results for: mmlu_professional_psychology\n",
      "2025-07-04:07:23:54,212 INFO     [lm_eval.loggers.evaluation_tracker:290] Saving per-sample results for: mmlu_public_relations\n",
      "2025-07-04:07:23:54,219 INFO     [lm_eval.loggers.evaluation_tracker:290] Saving per-sample results for: mmlu_security_studies\n",
      "2025-07-04:07:23:54,244 INFO     [lm_eval.loggers.evaluation_tracker:290] Saving per-sample results for: mmlu_sociology\n",
      "2025-07-04:07:23:54,258 INFO     [lm_eval.loggers.evaluation_tracker:290] Saving per-sample results for: mmlu_us_foreign_policy\n",
      "2025-07-04:07:23:54,265 INFO     [lm_eval.loggers.evaluation_tracker:290] Saving per-sample results for: mmlu_virology\n",
      "2025-07-04:07:23:54,275 INFO     [lm_eval.loggers.evaluation_tracker:290] Saving per-sample results for: mmlu_world_religions\n",
      "2025-07-04:07:23:54,285 INFO     [lm_eval.loggers.evaluation_tracker:290] Saving per-sample results for: wmdp_bio\n",
      "2025-07-04:07:23:54,375 INFO     [lm_eval.loggers.evaluation_tracker:290] Saving per-sample results for: wmdp_chem\n",
      "2025-07-04:07:23:54,400 INFO     [lm_eval.loggers.evaluation_tracker:290] Saving per-sample results for: wmdp_cyber\n",
      "hf (pretrained=/home/ubuntu/thesis/sae/results/models/sae_steer,dtype=bfloat16), gen_kwargs: (None), limit: None, num_fewshot: None, batch_size: 8\n",
      "|                 Tasks                 |Version|Filter|n-shot|Metric|   |Value |   |Stderr|\n",
      "|---------------------------------------|------:|------|-----:|------|---|-----:|---|-----:|\n",
      "|mmlu                                   |      2|none  |      |acc   |↑  |0.4794|±  |0.0041|\n",
      "| - humanities                          |      2|none  |      |acc   |↑  |0.4351|±  |0.0070|\n",
      "|  - formal_logic                       |      1|none  |     0|acc   |↑  |0.3175|±  |0.0416|\n",
      "|  - high_school_european_history       |      1|none  |     0|acc   |↑  |0.5758|±  |0.0386|\n",
      "|  - high_school_us_history             |      1|none  |     0|acc   |↑  |0.5931|±  |0.0345|\n",
      "|  - high_school_world_history          |      1|none  |     0|acc   |↑  |0.6245|±  |0.0315|\n",
      "|  - international_law                  |      1|none  |     0|acc   |↑  |0.6116|±  |0.0445|\n",
      "|  - jurisprudence                      |      1|none  |     0|acc   |↑  |0.5093|±  |0.0483|\n",
      "|  - logical_fallacies                  |      1|none  |     0|acc   |↑  |0.5521|±  |0.0391|\n",
      "|  - moral_disputes                     |      1|none  |     0|acc   |↑  |0.5173|±  |0.0269|\n",
      "|  - moral_scenarios                    |      1|none  |     0|acc   |↑  |0.2369|±  |0.0142|\n",
      "|  - philosophy                         |      1|none  |     0|acc   |↑  |0.5338|±  |0.0283|\n",
      "|  - prehistory                         |      1|none  |     0|acc   |↑  |0.5833|±  |0.0274|\n",
      "|  - professional_law                   |      1|none  |     0|acc   |↑  |0.3703|±  |0.0123|\n",
      "|  - world_religions                    |      1|none  |     0|acc   |↑  |0.6433|±  |0.0367|\n",
      "| - other                               |      2|none  |      |acc   |↑  |0.5388|±  |0.0087|\n",
      "|  - business_ethics                    |      1|none  |     0|acc   |↑  |0.4400|±  |0.0499|\n",
      "|  - clinical_knowledge                 |      1|none  |     0|acc   |↑  |0.5208|±  |0.0307|\n",
      "|  - college_medicine                   |      1|none  |     0|acc   |↑  |0.4566|±  |0.0380|\n",
      "|  - global_facts                       |      1|none  |     0|acc   |↑  |0.3100|±  |0.0465|\n",
      "|  - human_aging                        |      1|none  |     0|acc   |↑  |0.5785|±  |0.0331|\n",
      "|  - management                         |      1|none  |     0|acc   |↑  |0.6019|±  |0.0485|\n",
      "|  - marketing                          |      1|none  |     0|acc   |↑  |0.7350|±  |0.0289|\n",
      "|  - medical_genetics                   |      1|none  |     0|acc   |↑  |0.5900|±  |0.0494|\n",
      "|  - miscellaneous                      |      1|none  |     0|acc   |↑  |0.6284|±  |0.0173|\n",
      "|  - nutrition                          |      1|none  |     0|acc   |↑  |0.5784|±  |0.0283|\n",
      "|  - professional_accounting            |      1|none  |     0|acc   |↑  |0.3582|±  |0.0286|\n",
      "|  - professional_medicine              |      1|none  |     0|acc   |↑  |0.4154|±  |0.0299|\n",
      "|  - virology                           |      1|none  |     0|acc   |↑  |0.4639|±  |0.0388|\n",
      "| - social sciences                     |      2|none  |      |acc   |↑  |0.5479|±  |0.0088|\n",
      "|  - econometrics                       |      1|none  |     0|acc   |↑  |0.2719|±  |0.0419|\n",
      "|  - high_school_geography              |      1|none  |     0|acc   |↑  |0.5202|±  |0.0356|\n",
      "|  - high_school_government_and_politics|      1|none  |     0|acc   |↑  |0.6477|±  |0.0345|\n",
      "|  - high_school_macroeconomics         |      1|none  |     0|acc   |↑  |0.4308|±  |0.0251|\n",
      "|  - high_school_microeconomics         |      1|none  |     0|acc   |↑  |0.4790|±  |0.0324|\n",
      "|  - high_school_psychology             |      1|none  |     0|acc   |↑  |0.6550|±  |0.0204|\n",
      "|  - human_sexuality                    |      1|none  |     0|acc   |↑  |0.5878|±  |0.0432|\n",
      "|  - professional_psychology            |      1|none  |     0|acc   |↑  |0.4706|±  |0.0202|\n",
      "|  - public_relations                   |      1|none  |     0|acc   |↑  |0.5636|±  |0.0475|\n",
      "|  - security_studies                   |      1|none  |     0|acc   |↑  |0.5714|±  |0.0317|\n",
      "|  - sociology                          |      1|none  |     0|acc   |↑  |0.7214|±  |0.0317|\n",
      "|  - us_foreign_policy                  |      1|none  |     0|acc   |↑  |0.7600|±  |0.0429|\n",
      "| - stem                                |      2|none  |      |acc   |↑  |0.4202|±  |0.0087|\n",
      "|  - abstract_algebra                   |      1|none  |     0|acc   |↑  |0.2900|±  |0.0456|\n",
      "|  - anatomy                            |      1|none  |     0|acc   |↑  |0.5407|±  |0.0430|\n",
      "|  - astronomy                          |      1|none  |     0|acc   |↑  |0.4803|±  |0.0407|\n",
      "|  - college_biology                    |      1|none  |     0|acc   |↑  |0.5556|±  |0.0416|\n",
      "|  - college_chemistry                  |      1|none  |     0|acc   |↑  |0.3700|±  |0.0485|\n",
      "|  - college_computer_science           |      1|none  |     0|acc   |↑  |0.3900|±  |0.0490|\n",
      "|  - college_mathematics                |      1|none  |     0|acc   |↑  |0.3300|±  |0.0473|\n",
      "|  - college_physics                    |      1|none  |     0|acc   |↑  |0.4020|±  |0.0488|\n",
      "|  - computer_security                  |      1|none  |     0|acc   |↑  |0.5000|±  |0.0503|\n",
      "|  - conceptual_physics                 |      1|none  |     0|acc   |↑  |0.4426|±  |0.0325|\n",
      "|  - electrical_engineering             |      1|none  |     0|acc   |↑  |0.5241|±  |0.0416|\n",
      "|  - elementary_mathematics             |      1|none  |     0|acc   |↑  |0.3333|±  |0.0243|\n",
      "|  - high_school_biology                |      1|none  |     0|acc   |↑  |0.5774|±  |0.0281|\n",
      "|  - high_school_chemistry              |      1|none  |     0|acc   |↑  |0.3941|±  |0.0344|\n",
      "|  - high_school_computer_science       |      1|none  |     0|acc   |↑  |0.4500|±  |0.0500|\n",
      "|  - high_school_mathematics            |      1|none  |     0|acc   |↑  |0.3222|±  |0.0285|\n",
      "|  - high_school_physics                |      1|none  |     0|acc   |↑  |0.3775|±  |0.0396|\n",
      "|  - high_school_statistics             |      1|none  |     0|acc   |↑  |0.3472|±  |0.0325|\n",
      "|  - machine_learning                   |      1|none  |     0|acc   |↑  |0.3661|±  |0.0457|\n",
      "|wmdp                                   |      1|none  |      |acc   |↑  |0.4029|±  |0.0080|\n",
      "| - wmdp_bio                            |      1|none  |     0|acc   |↑  |0.5161|±  |0.0140|\n",
      "| - wmdp_chem                           |      1|none  |     0|acc   |↑  |0.3701|±  |0.0239|\n",
      "| - wmdp_cyber                          |      1|none  |     0|acc   |↑  |0.3372|±  |0.0106|\n",
      "\n",
      "|      Groups      |Version|Filter|n-shot|Metric|   |Value |   |Stderr|\n",
      "|------------------|------:|------|------|------|---|-----:|---|-----:|\n",
      "|mmlu              |      2|none  |      |acc   |↑  |0.4794|±  |0.0041|\n",
      "| - humanities     |      2|none  |      |acc   |↑  |0.4351|±  |0.0070|\n",
      "| - other          |      2|none  |      |acc   |↑  |0.5388|±  |0.0087|\n",
      "| - social sciences|      2|none  |      |acc   |↑  |0.5479|±  |0.0088|\n",
      "| - stem           |      2|none  |      |acc   |↑  |0.4202|±  |0.0087|\n",
      "|wmdp              |      1|none  |      |acc   |↑  |0.4029|±  |0.0080|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!uv run lm-eval --model hf \\\n",
    "    --model_args pretrained=\"/home/ubuntu/thesis/sae/results/models/sae_steer\",dtype=bfloat16 \\\n",
    "    --tasks mmlu,wmdp \\\n",
    "    --log_samples \\\n",
    "    --output_path \"eval/\" \\\n",
    "    --batch_size=8 \\\n",
    "    --seed 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f49952c5-941e-4e3e-9d9b-ed89e43e5476",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
